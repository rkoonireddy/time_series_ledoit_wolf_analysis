{"cells":[{"cell_type":"markdown","source":["### **assumptions about data used by Ledoit and Wolf**\n","1. 21 consecutive days constitute one month\n","2. 01.10.1972 to 12.31.2011 data from CSRP is used. (daily stock returns data)\n","3. Out of sample is considered to be from 01.19.1973 to 12.31.11 (480 months)\n","4. Portfolios are updated monthly\n","5. At any investment date h, a coviariance matrix is estimated using the most recent T = 250 daily returns\n","6. There are a total of 480 months"],"metadata":{"id":"7FX8wakRAhxK"}},{"cell_type":"markdown","metadata":{"id":"vfqjakSSPGJl"},"source":["#steps\n","1. Consider the given data as population data of all the stocks - we have about 10800 data points for 184 tickers of S&P 500 between 27-10-1981 and 31-12-2022\n","2. Take blocks of these data and evaluate the assumptions of multiple models as provided by Ledoit and Wolf in the paper Large Dimensional Covariance Matrix Shrinkage\n","3. Compare results and find significance of these results on an average\n"]},{"cell_type":"code","source":["import scipy as sp\n","from scipy.optimize import minimize\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","from matplotlib import gridspec\n","from datetime import datetime, timedelta\n","import os\n","import sys\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"metadata":{"id":"C30VIt8W1BY_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5591,"status":"ok","timestamp":1682946469316,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"},"user_tz":-120},"id":"mV29sMp1a0a4","outputId":"3984e012-a3cd-4b25-b908-f5359b894d81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nonlinshrink in /usr/local/lib/python3.10/dist-packages (0.7)\n"]}],"source":["!pip install nonlinshrink"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2474,"status":"ok","timestamp":1682946471774,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"},"user_tz":-120},"id":"KJIBJsW5Kv5R","outputId":"775e62ec-0994-47d4-99e8-100eed8c86e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#import drive and change folders to current folder\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"/content/drive/MyDrive/time_series_analysis_2023/final_implementation\")"]},{"cell_type":"markdown","metadata":{"id":"WkDp8_l9iI3d"},"source":["### load data from pre-populated files, you can also get data from yahoo finance from same dates"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DoSDeT_GKujW"},"outputs":[],"source":["#read data\n","#dates are from 27-10 not 27-01\n","import pandas as pd \n","prices = pd.read_csv(\"SandP500_27-01-1981_31-12-2022.csv\")\n","# print(f\"stock prices:\\n {prices.head()}\")\n","# print(f\"stock prices shape:\\n {prices.shape}\")\n","\n","returns = pd.read_csv(\"SandP500_returns_percentage_27-01-1981_31-12-2022.csv\")\n","# print(f\"stock percentage returns:\\n {returns.head()}\")\n","\n","returns_demeaned = pd.read_csv(\"SandP500_returns_percentage_demeaned_27-01-1981_31-12-2022.csv\")\n","# print(f\"stock percentage demeanend returns:\\n {returns_demeaned.head()}\")\n","\n","log_returns = pd.read_csv(\"SandP500_log_returns_27-01-1981_31-12-2022.csv\")\n","# print(f\"stock log returns:\\n {log_returns.head()}\")\n","\n","log_returns_demeaned = pd.read_csv(\"SandP500_returns_log_returns_demeaned_27-01-1981_31-12-2022.csv\")\n","# print(f\"log returns demeaned:\\n {log_returns_demeaned.head()}\")\n","\n","\n","sp_returns_single = pd.read_csv(\"sp500_single_log_returns_27-01-1981_31-12-2022.csv\")\n","# print(f\"sp500 whole returns:\\n {sp_returns_single.head()}\")\n","\n","ff_data = pd.read_csv(\"ff_daily_data.csv\")\n","# print(f\"fama french factors:\\n {ff_data.head()}\")\n","\n","from datetime import datetime\n","def convert_date(date_string):\n","    date = datetime.strptime(date_string, \"%Y-%m-%d\") \n","    return int(date.strftime(\"%Y%m%d\"))\n","\n","def convert_to_date(date_string):\n","    date_string = str(date_string)\n","    formatted_date = f\"{date_string[:4]}-{date_string[4:6]}-{date_string[6:]}\"\n","    return formatted_date\n","\n","#change all dates to the fama french date format\n","prices.date = prices.date.apply(convert_date)\n","returns.date = returns.date.apply(convert_date)\n","returns_demeaned.date = returns_demeaned.date.apply(convert_date)\n","log_returns.date = log_returns.date.apply(convert_date)\n","log_returns_demeaned.date = log_returns_demeaned.date.apply(convert_date)\n","sp_returns_single.date = sp_returns_single.date.apply(convert_date)\n","ff_data = ff_data[ff_data['date'].isin(log_returns.date)].reset_index(drop=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JPkBqyq0qeuk"},"outputs":[],"source":["#print samples\n","print(\"prices:\\n\",prices.head(2))\n","print(\"returns:\\n\",returns.head(2))\n","print(\"returns_demeaned:\\n\",returns_demeaned.head(2))\n","print(\"log_returns:\\n\",log_returns.head(2))\n","print(\"log_returns_demeaned:\\n\",log_returns_demeaned.head(2))\n","print(\"sp_returns_single:\\n\",sp_returns_single.head(2))\n","print(\"ff_data:\\n\",ff_data.head(2))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1682946487714,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"},"user_tz":-120},"id":"K_edEocSrqZv","outputId":"ae89a76a-12db-4783-b387-0ed44f58f313"},"outputs":[{"output_type":"stream","name":"stdout","text":["total_universes: 482\n","total_months given: 494.3809523809524\n","number of days required for 494 months:10374\n"]}],"source":["month_trading_days = 21\n","year_trading_days = month_trading_days*12\n","total_months = returns.shape[0]/21\n","#total universes is months starting from 1 year after the beginning date\n","total_universes = int((returns.shape[0]-year_trading_days)/month_trading_days)\n","print(f\"total_universes: { total_universes}\")\n","print(f\"total_months given: { total_months}\")\n","print(f\"number of days required for 494 months:{21*494}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":375,"status":"ok","timestamp":1682929512314,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"},"user_tz":-120},"id":"pq5XD59u5svD","outputId":"685f4401-a93b-4568-ccf2-a8369c6a42f5"},"outputs":[{"data":{"text/plain":["Index([], dtype='object')"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["log_returns.columns[prices.isnull().any()]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zG-FbiyWgvaV"},"outputs":[],"source":["#modify this <- i use percentage changes everywhere. \n","#I discard logarithmic changes due to the inability to use them in factor models\n","log_returns = pd.read_csv(\"SandP500_returns_percentage_27-01-1981_31-12-2022.csv\")\n","log_returns.date = log_returns.date.apply(convert_date)\n","\n","# print(f\"stock percentage returns:\\n {returns.head()}\")"]},{"cell_type":"markdown","metadata":{"id":"wNm3W8Eti8BI"},"source":["## run below block for all covariance calculation functions "]},{"cell_type":"markdown","metadata":{"id":"KyV0QznE8fDH"},"source":["## **list all ways to compute covariance matrices**\n","\n","-- defined functions\n","1. *Ledoit-Wolf libaries*\n","  1. *Linear Shrinkage*\n","      1. **cov1Para** (Covariance 1 parameter -> variances same and covariances to zero)\n","      2. **cov2Para** (2 parameter model - all variance are same and all covarainces are same)\n","      3. **covCor** (Constant-correlation matrix;)\n","      4. **covDiag** -> (preserves diagonal of sample)\n","      5. **covMarket** -> (shrinkage towards one factor model)\n","\n","   2. Non-Linear Shrinkage\n","      1. **LIS** (linear inverse shrinkage)\n","      2. **QIS** (Quadratic inverse shrinkage)\n","      3. **GIS** (Geometric inverse shrinkage) \n","\n","2. From SkLearn\n","  1. **LedoitWolf**\n","  2. **ShrunkCovariance**\n","  3. **OAS** (Oracle Approximate Shrinkage) \n","\n","3. Own/ Collected Built libaries\n","  1. **equal_weight** (equal weightage) -> sigma becomes 1\n","  2. **sample_cov** (takes covariance of sample)\n","  3. **lin_shrink** (use linear shrinkage from sklearn's \n","  4. **non_lin_shrink**\n","  5. **single_factor**\n","  6. **FamaFrench**\n","  7. **POET**\n","  8. **NLSF**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCr4acEjLsjI"},"outputs":[],"source":["#get functions from ledoit-wolf\n","#Imports\n","import numpy as np\n","import pandas as pd\n","import math\n","\n","\n","# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Oct  3 17:59:28 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","\n","#Sigmahat function\n","def GIS(Y,k=None):\n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","\n","    #Set df dimensions\n","    N = Y.shape[0]                                              #num of columns\n","    p = Y.shape[1]                                                 #num of rows\n","\n","    #default setting\n","    if (k is None or math.isnan(k)):\n","        Y = Y.sub(Y.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                      # adjust effective sample size\n","    c = p/n                                               # concentration ratio\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    sample = (sample+sample.T)/2                              #make symmetrical\n","\n","    #Spectral decomp\n","    lambda1, u = np.linalg.eigh(sample)            #use Cholesky factorisation \n","    #                                               based on hermitian matrix\n","    lambda1 = lambda1.real.clip(min=0)              #reset negative values to 0\n","    dfu = pd.DataFrame(u,columns=lambda1)   #create df with column names lambda\n","    #                                        and values u\n","    dfu.sort_index(axis=1,inplace = True)              #sort df by column index\n","    lambda1 = dfu.columns                              #recapture sorted lambda\n","\n","    #COMPUTE Quadratic-Inverse Shrinkage estimator of the covariance matrix\n","    h = (min(c**2,1/c**2)**0.35)/p**0.35                   #smoothing parameter\n","    invlambda = 1/lambda1[max(1,p-n+1)-1:p]  #inverse of (non-null) eigenvalues\n","    dfl = pd.DataFrame()\n","    dfl['lambda'] = invlambda\n","    Lj = dfl[np.repeat(dfl.columns.values,min(p,n))]          #like  1/lambda_j\n","    Lj = pd.DataFrame(Lj.to_numpy())                        #Reset column names\n","    Lj_i = Lj.subtract(Lj.T)                    #like (1/lambda_j)-(1/lambda_i)\n","   \n","    theta = Lj.multiply(Lj_i).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)          #smoothed Stein shrinker\n","    Htheta = Lj.multiply(Lj*h).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)                    #its conjugate\n","    Atheta2 = theta**2+Htheta**2                         #its squared amplitude\n","    \n","    if p<=n:               #case where sample covariance matrix is not singular\n","        deltahat_1=(1-c)*invlambda+2*c*invlambda*theta #shrunk inverse eigenvalues (LIS)\n","        \n","        delta = 1 / ((1-c)**2*invlambda+2*c*(1-c)*invlambda*theta \\\n","                      +c**2*invlambda*Atheta2)    #optimally shrunk eigenvalues\n","        delta = delta.to_numpy()\n","    else: # case where sample covariance matrix is singular\n","        print('p must be <= n for the Symmetrized Kullback-Leibler divergence')       \n","        return -1\n","    \n","    temp = pd.DataFrame(deltahat_1)\n","    x = min(invlambda)\n","    temp.loc[temp[0] < x, 0] = x\n","    deltaLIS_1 = temp[0]\n","\n","    temp1 = dfu.to_numpy()\n","    temp2 = np.diag((delta/deltaLIS_1)**0.5)\n","    temp3 = dfu.T.to_numpy().conjugate()\n","    # reconstruct covariance matrix\n","    sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1,temp2),temp3))\n","    \n","    return sigmahat\n","\n","\n","#Sigmahat function\n","def LIS(Y,k=None):\n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","\n","    #Set df dimensions\n","    N = Y.shape[0]                                              #num of columns\n","    p = Y.shape[1]                                                 #num of rows\n","\n","    #default setting\n","    if (k is None or math.isnan(k)):\n","        Y = Y.sub(Y.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                      # adjust effective sample size\n","    c = p/n                                               # concentration ratio\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    sample = (sample+sample.T)/2                              #make symmetrical\n","\n","    #Spectral decomp\n","    lambda1, u = np.linalg.eigh(sample)            #use Cholesky factorisation \n","    #                                               based on hermitian matrix\n","    lambda1 = lambda1.real.clip(min=0)              #reset negative values to 0\n","    dfu = pd.DataFrame(u,columns=lambda1)   #create df with column names lambda\n","    #                                        and values u\n","    dfu.sort_index(axis=1,inplace = True)              #sort df by column index\n","    lambda1 = dfu.columns                              #recapture sorted lambda\n","\n","    #COMPUTE Quadratic-Inverse Shrinkage estimator of the covariance matrix\n","    h = (min(c**2,1/c**2)**0.35)/p**0.35                   #smoothing parameter\n","    invlambda = 1/lambda1[max(1,p-n+1)-1:p]  #inverse of (non-null) eigenvalues\n","    dfl = pd.DataFrame()\n","    dfl['lambda'] = invlambda\n","    Lj = dfl[np.repeat(dfl.columns.values,min(p,n))]          #like  1/lambda_j\n","    Lj = pd.DataFrame(Lj.to_numpy())                        #Reset column names\n","    Lj_i = Lj.subtract(Lj.T)                    #like (1/lambda_j)-(1/lambda_i)\n","   \n","    theta = Lj.multiply(Lj_i).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)          #smoothed Stein shrinker\n","    \n","    if p<=n:               #case where sample covariance matrix is not singular\n","         deltahat_1=(1-c)*invlambda+2*c*invlambda*theta #shrunk inverse eigenvalues\n","         \n","    else: # case where sample covariance matrix is singular\n","        print(\"p must be <= n for Stein's loss\")       \n","        return -1\n","    \n","    temp = pd.DataFrame(deltahat_1)\n","    x = min(invlambda)\n","    temp.loc[temp[0] < x, 0] = x\n","    deltaLIS_1 = temp[0]\n","\n","\n","    temp1 = dfu.to_numpy()\n","    temp2 = np.diag(1/deltaLIS_1)\n","    temp3 = dfu.T.to_numpy().conjugate()\n","    # reconstruct covariance matrix\n","    sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1,temp2),temp3))\n","    \n","    return sigmahat\n","\n","def QIS(Y,k=None):\n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","\n","    #Set df dimensions\n","    N = Y.shape[0]                                              #num of columns\n","    p = Y.shape[1]                                                 #num of rows\n","\n","    #default setting\n","    if (k is None or math.isnan(k)):\n","        Y = Y.sub(Y.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                      # adjust effective sample size\n","    c = p/n                                               # concentration ratio\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    sample = (sample+sample.T)/2                              #make symmetrical\n","\n","    #Spectral decomp\n","    lambda1, u = np.linalg.eigh(sample)            #use Cholesky factorisation \n","    #                                               based on hermitian matrix\n","    lambda1 = lambda1.real.clip(min=0)              #reset negative values to 0\n","    dfu = pd.DataFrame(u,columns=lambda1)   #create df with column names lambda\n","    #                                        and values u\n","    dfu.sort_index(axis=1,inplace = True)              #sort df by column index\n","    lambda1 = dfu.columns                              #recapture sorted lambda\n","\n","    #COMPUTE Quadratic-Inverse Shrinkage estimator of the covariance matrix\n","    h = (min(c**2,1/c**2)**0.35)/p**0.35                   #smoothing parameter\n","    invlambda = 1/lambda1[max(1,p-n+1)-1:p]  #inverse of (non-null) eigenvalues\n","    dfl = pd.DataFrame()\n","    dfl['lambda'] = invlambda\n","    Lj = dfl[np.repeat(dfl.columns.values,min(p,n))]          #like  1/lambda_j\n","    Lj = pd.DataFrame(Lj.to_numpy())                        #Reset column names\n","    Lj_i = Lj.subtract(Lj.T)                    #like (1/lambda_j)-(1/lambda_i)\n","   \n","    theta = Lj.multiply(Lj_i).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)          #smoothed Stein shrinker\n","    Htheta = Lj.multiply(Lj*h).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)                    #its conjugate\n","    Atheta2 = theta**2+Htheta**2                         #its squared amplitude\n","\n","    if p<=n:               #case where sample covariance matrix is not singular\n","         delta = 1 / ((1-c)**2*invlambda+2*c*(1-c)*invlambda*theta \\\n","                      +c**2*invlambda*Atheta2)    #optimally shrunk eigenvalues\n","         delta = delta.to_numpy()\n","    else:\n","        delta0 = 1/((c-1)*np.mean(invlambda.to_numpy())) #shrinkage of null \n","        #                                                 eigenvalues\n","        delta = np.repeat(delta0,p-n)\n","        delta = np.concatenate((delta, 1/(invlambda*Atheta2)), axis=None)\n","\n","    deltaQIS = delta*(sum(lambda1)/sum(delta))                  #preserve trace\n","    \n","    temp1 = dfu.to_numpy()\n","    temp2 = np.diag(deltaQIS)\n","    temp3 = dfu.T.to_numpy().conjugate()\n","    #reconstruct covariance matrix\n","    sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1,temp2),temp3))\n","    return sigmahat\n","\n","def cov1Para(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","    \n","    \n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    \n","    \n","    # compute shrinkage target\n","    diag = np.diag(sample.to_numpy())\n","    meanvar= sum(diag)/len(diag)\n","    target=meanvar*np.eye(p)\n","    \n","    \n","    \n","    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    \n","    \n","    pihat = sum(piMat.sum())\n","    \n","\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag=0;\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    rho_off=0;\n","    \n","    # compute shrinkage intensity\n","    rhohat=rho_diag+rho_off\n","    kappahat=(pihat-rhohat)/gammahat\n","    shrinkage=max(0,min(1,kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat=shrinkage*target+(1-shrinkage)*sample\n","    \n","    \n","    return sigmahat\n","\n","def cov2Para(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","    \n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    \n","    \n","    #compute shrinkage target\n","    diag = np.diag(sample.to_numpy())\n","    meanvar= sum(diag)/len(diag)\n","    meancov = (np.sum(sample.to_numpy()) - np.sum(np.eye(p)*sample.to_numpy()))/(p*(p-1));\n","    target = pd.DataFrame(meanvar*np.eye(p)+meancov*(1-np.eye(p)))\n","    \n","    #estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    pihat = sum(piMat.sum())\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag = (sample2.sum().sum()-np.trace(sample.to_numpy())**2)/p;\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    sum1=Y.sum(axis=1)\n","    sum2=Y2.sum(axis=1)\n","    temp = (np.multiply(sum1.to_numpy(),sum1.to_numpy())-sum2)\n","    rho_off1 = np.sum(np.multiply(temp,temp))/(p*n)\n","    rho_off2 = (sample.sum().sum()-np.trace(sample.to_numpy()))**2/p\n","    rho_off = (rho_off1-rho_off2)/(p-1)\n","    \n","    # compute shrinkage intensity\n","    rhohat = rho_diag + rho_off\n","    kappahat = (pihat-rhohat) / gammahat\n","    shrinkage = max(0 , min(1 , kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat=shrinkage*target+(1-shrinkage)*sample\n","    \n","    return sigmahat\n","\n","\n","def covCor(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import numpy.matlib as mt\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","        \n","    # compute shrinkage target\n","    samplevar = np.diag(sample.to_numpy())\n","    sqrtvar = pd.DataFrame(np.sqrt(samplevar))\n","    rBar = (np.sum(np.sum(sample.to_numpy()/np.matmul(sqrtvar.to_numpy(),sqrtvar.T.to_numpy())))-p)/(p*(p-1)) # mean correlation\n","    target = pd.DataFrame(rBar*np.matmul(sqrtvar.to_numpy(),sqrtvar.T.to_numpy()))\n","    target[np.logical_and(np.eye(p),np.eye(p))] = sample[np.logical_and(np.eye(p),np.eye(p))];\n","    \n","    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    pihat = sum(piMat.sum())\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag =  np.sum(np.diag(piMat))\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    term1 = pd.DataFrame(np.matmul((Y**3).T.to_numpy(),Y.to_numpy())/n)\n","    term2 = pd.DataFrame(np.transpose(mt.repmat(samplevar,p,1))*sample)\n","    thetaMat = term1-term2\n","    thetaMat[np.logical_and(np.eye(p),np.eye(p))] = pd.DataFrame(np.zeros((p,p)))[np.logical_and(np.eye(p),np.eye(p))]\n","    rho_off = rBar*(np.matmul((1/sqrtvar).to_numpy(),sqrtvar.T.to_numpy())*thetaMat).sum().sum()\n","    \n","    # compute shrinkage intensity\n","    rhohat = rho_diag + rho_off\n","    kappahat = (pihat - rhohat) / gammahat\n","    shrinkage = max(0 , min(1 , kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat = shrinkage*target + (1-shrinkage) * sample;\n","    \n","    return sigmahat\n","\n","\n","def covDiag(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","        \n","    # compute shrinkage target\n","    target = pd.DataFrame(np.diag(np.diag(sample.to_numpy())))\n","    \n","    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    pihat = sum(piMat.sum())\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag =  np.sum(np.diag(piMat))\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    rho_off = 0\n","    \n","    # compute shrinkage intensity\n","    rhohat = rho_diag + rho_off\n","    kappahat = (pihat - rhohat) / gammahat\n","    shrinkage = max(0 , min(1 , kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat = shrinkage*target + (1-shrinkage) * sample;\n","    \n","    return sigmahat\n","\n","def covMarket(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import numpy.matlib as mt\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","    \n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","\n","    #compute shrinkage target\n","    Ymkt = Y.mean(axis = 1) #equal-weighted market factor\n","    covmkt = pd.DataFrame(np.matmul(Y.T.to_numpy(),Ymkt.to_numpy()))/n #covariance of original variables with common factor\n","    varmkt = np.matmul(Ymkt.T.to_numpy(),Ymkt.to_numpy())/n #variance of common factor\n","    target = pd.DataFrame(np.matmul(covmkt.to_numpy(),covmkt.T.to_numpy()))/varmkt\n","    target[np.logical_and(np.eye(p),np.eye(p))] = sample[np.logical_and(np.eye(p),np.eye(p))]\n","    \n","    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    pihat = sum(piMat.sum())\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag =  np.sum(np.diag(piMat))\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    temp = pd.DataFrame(Y.values*(pd.DataFrame([Ymkt for i in range(p)]).T.values)) #this must perform element wise multiplication but not performing #resolved\n","    covmktSQ = pd.DataFrame([covmkt[0] for i in range(p)])\n","    v1 = pd.DataFrame((1/n) * np.matmul(Y2.T.to_numpy(),temp.to_numpy())-np.multiply(covmktSQ.T.to_numpy(),sample.to_numpy()))\n","    roff1 = (np.sum(np.sum(np.multiply(v1.to_numpy(),covmktSQ.to_numpy())))-np.sum(np.diag(np.multiply(v1.to_numpy(),covmkt.to_numpy()))))/varmkt\n","    v3 = pd.DataFrame((1/n) * np.matmul(temp.T.to_numpy(),temp.to_numpy()) - varmkt * sample)\n","    roff3 = (np.sum(np.sum(np.multiply(v3.to_numpy(),np.matmul(covmkt.to_numpy(),covmkt.T.to_numpy())))) - np.sum(np.multiply(np.diag(v3.to_numpy()),(covmkt[0]**2).to_numpy()))) /varmkt**2\n","    rho_off=2*roff1-roff3\n","    \n","    # compute shrinkage intensity\n","    rhohat = rho_diag + rho_off\n","    kappahat = (pihat - rhohat) / gammahat\n","    shrinkage = max(0 , min(1 , kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat = shrinkage*target + (1-shrinkage) * sample;\n","    \n","    return sigmahat\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9aHOxojDinHG"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import nonlinshrink as nls\n","import time\n","\n","#define all the other covariances\n","def equal_weight(log_ret, k=None):\n","    #default setting\n","    import numpy as np\n","    if  k is None or math.isnan(k):\n","        log_ret = log_ret.sub(log_ret.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","    return np.identity(log_ret.shape[1])\n","\n","def sample_cov(log_ret,k=None):\n","    #default setting\n","    import numpy as np\n","    if  k is None or math.isnan(k):\n","        log_ret = log_ret.sub(log_ret.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","    return np.cov(log_ret.T)\n","\n","def lin_shrink(log_ret,k=None):\n","    #default setting\n","    import numpy as np\n","    if  k is None or math.isnan(k):\n","        log_ret = log_ret.sub(log_ret.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","    from sklearn.covariance import ledoit_wolf\n","    return ledoit_wolf(log_ret)[0]   \n","\n","def non_lin_shrink(log_ret,k=None):\n","    #default setting\n","    import numpy as np\n","    import nonlinshrink as nls\n","    if  k is None or math.isnan(k):\n","        log_ret = log_ret.sub(log_ret.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","    return nls.shrink_cov(log_ret)      \n","\n","def single_factor(log_ret,k=None):\n","    #default setting\n","    import numpy as np\n","    import nonlinshrink as nls\n","    if  k is None or math.isnan(k):\n","        log_ret = log_ret.sub(log_ret.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","    \n","    #Generate Factor\n","    equalw=np.array([[1/log_ret.shape[1]]*log_ret.shape[1]])\n","    # print(log_ret.shape)\n","    # print(equalw.shape)\n","    factor=equalw@log_ret.T\n","    # print(factor)\n","\n","    #estimate SigmaF\n","    var_f=np.var(factor,ddof=1)      #variance of factor,\n","    np.savetxt(\"SF_factor.csv\",factor,delimiter=\",\")\n","\n","    #compute cov(Ri,Rf),the covariance of stocks and factor\n","    var_if=np.cov(log_ret.T,factor)[-1,:-1]\n","    var_if=np.matrix(var_if)          #convert to 1X100 matrix\n","    # print(\"var_if\",var_if.shape)\n","\n","    SigmaSF = (var_if.T*var_if)/log_ret.shape[1]\n","    # print(\"SigmaSF\",SigmaSF.shape)\n","    for i in range(log_ret.shape[1]):\n","        SigmaSF[i,i]=np.cov(log_ret.T)[i,i]\n","\n","    return SigmaSF\n","\n","def FamaFrench(log_ret,FFfactors,k=None):\n","\n","    import numpy as np\n","    if  k is None or math.isnan(k):\n","        log_ret = log_ret.sub(log_ret.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","\n","    from sklearn.linear_model import LinearRegression\n","    LG=LinearRegression()\n","    ### Generate 3-factors array.\n","    LG.fit(FFfactors,log_ret)      ##FFfactor matirx is a (250,3) matrix!\n","    betas=LG.coef_\n","\n","    var_ff=np.cov(FFfactors.T)      ##Covariance of FAMA FRENCH 3 Factor model.\n","\n","    SigmaF=betas@var_ff@betas.T\n","\n","    ###As same as SF, the diagonal need add residual,or replace by var(Ri)\n","    for i in range(log_ret.shape[1]):\n","        SigmaF[i,i]=np.cov(log_ret.T)[i,i]\n","\n","    return SigmaF\n","\n","def POET(log_ret,k=None):\n","    from sklearn.decomposition import PCA\n","    import numpy as np\n","    if  k is None or math.isnan(k):\n","        log_ret = log_ret.sub(log_ret.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","\n","    pca = PCA(n_components=5, copy=True)\n","    pca.fit(log_ret.T)\n","    factors=pca.components_\n","\n","\n","    ##Regression:\n","    from sklearn.linear_model import LinearRegression\n","    LG2=LinearRegression()\n","\n","    LG2.fit(factors.T,log_ret)\n","    betas=LG2.coef_\n","\n","    var_fs=np.cov(factors)\n","\n","    SigmaF=betas@var_fs@betas.T\n","\n","    ###As same as SF, the diagonal need add residual,or replace by var(Ri)\n","\n","    for i in range(log_ret.shape[1]):\n","        SigmaF[i,i]=np.cov(log_ret.T)[i,i]\n","        \n","    return SigmaF\n","\n","def NLSF(log_ret,k=None):\n","    import numpy as np\n","    if  k is None or math.isnan(k):\n","        log_ret = log_ret.sub(log_ret.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","    \n","    SigmaSF = single_factor(log_ret)\n","\n","    ## NL-SF\n","    eigenvalue, eigenvectors = np.linalg.eig(SigmaSF)\n","    \n","    diag=np.identity(log_ret.shape[1])\n","    diag2=np.zeros((log_ret.shape[1],log_ret.shape[1]))\n","    for i in range(log_ret.shape[1]):\n","        diag[i,i]=pow(eigenvalue[i],-1/2)\n","        diag2[i,i]=pow(eigenvalue[i],1/2)\n","    ##Generate Yt x Sigma_SF to the power of -1/2\n","    SigmaSF2=eigenvectors@diag@eigenvectors.T  ##(1/2)\n","    SigmaSF3=eigenvectors@diag2@eigenvectors.T  ##(-1/2)\n","    SigmaC_hat=nls.shrink_cov(log_ret@SigmaSF2)\n","\n","    #Reincorporating the structure.\n","    SigmaNLSF=SigmaSF3@SigmaC_hat@SigmaSF3\n","    return SigmaNLSF"]},{"cell_type":"markdown","metadata":{"id":"q5rXTYLliwmz"},"source":["## function based code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nazgjl_dvztt"},"outputs":[],"source":["def portfolio_return(mu, weights):\n","    '''\n","    Calculate expected portfolio returns for given weights\n","    '''\n","    if not len(mu) == len(weights):\n","        raise ValueError ('shape mismatch: weights and mu must be of same dimension')\n","        \n","    return np.dot(mu, weights)\n","\n","def portfolio_variance(sigma, weights, std=True):\n","    '''\n","    Calculate expected portfolio variance (std=False) or standard deviation (std=True)\n","    '''\n","    if not len(sigma) == len(weights):\n","        raise ValueError ('shape mismatch: weights and sigma must be of same dimension')\n","    \n","    # print(\"portfolio_variance\")\n","    # print(weights.T.shape)\n","    # print(np.array(np.dot(sigma , weights)).flatten().shape)\n","    variance = np.dot(weights.T, np.array(np.dot(sigma , weights)).flatten())\n","    \n","    if std:\n","        return np.sqrt(variance)\n","    else:\n","        return variance\n","\n","def sharpe_ratio(mu, sigma, weights):\n","    mu_pf = portfolio_return(mu, weights)\n","    std_pf = portfolio_variance(sigma, weights)\n","    return mu_pf/std_pf\n","\n","def portfolio_stats(mu, sigma, weights, annualize=True, print_out=True):\n","    if annualize:\n","        ann_factor = 252\n","        print_str = 'annual'\n","    else:\n","        ann_factor = 1 \n","        print_str = 'daily'\n","        \n","    mu_pf = portfolio_return(mu, weights)*ann_factor\n","    std_pf = portfolio_variance(sigma, weights)*np.sqrt(ann_factor)\n","    sharpe = mu_pf/std_pf\n","    \n","    if print_out:\n","        print('Expected {}: \\n return: {:.4f} \\n volatility: {:.4f} \\n Sharpe-ratio: {:.4f}'.format(\n","        print_str, mu_pf, std_pf, sharpe))\n","    \n","    return [mu_pf, std_pf, sharpe]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fn8_FT86hJJV"},"outputs":[],"source":["\"\"\"\n","functions for calculating weights\n","1. get_weight_allow_ss to allow for short selling when allocation (weights can be -ve)\n","2. get_weights_notallow_ss which doesnt allow short selling when allocation (weights are always >=0)\n","\"\"\"\n","#calculate weights <- using formula from ledoit wolf\n","def get_weights_allow_ss(covmat): \n","    covmat= np.matrix(covmat)\n","    \"\"\"\n","    calcultes the formula for global minimum variance portfolio given a covariance matrix of assets.\n","    this weight calculation allows for short selling\n","    \"\"\"\n","    #check\n","    if covmat.shape[0]!= covmat.shape[1]:\n","        print(\"Error : covariance matric is not a symmetric matrix, check code. \")\n","        raise SystemExit(1)\n","    else:\n","        weights = (covmat.I@np.ones((covmat.shape[0],1)))/(np.ones((1,covmat.shape[0]))@covmat.I@np.ones((covmat.shape[0],1)))\n","        return weights\n","\n","def get_weights_notallow_ss(mu, covmat):\n","    import scipy as sp\n","    from scipy.optimize import minimize\n","    '''\n","    In this weights method, I do not allow the short selling, i.e. weights are not allowed to be negative\n","    Mean-variance portfolio optimization with SLSQP\n","    \n","    Returns\n","        Optimal weights\n","    '''    \n","    if not isinstance(mu, np.ndarray):\n","        mu = np.array(mu)        \n","    \n","    def objective_f(weights):\n","        return - sharpe_ratio(mu, covmat, weights)    \n","    \n","    n = mu.size\n","    # print(\"n shape:\",n)\n","\n","    # Constraints: sum of weights = 1\n","    cons = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1.})\n","    # Boundaries: 0 =< weights =< 1 (only long positions)\n","    bnds = tuple((0, 1) for x in range(n))\n","    # initial guess: equal weights\n","    x_0 = np.repeat(1/n, n)\n","    # Sequential Least SQuares Programming (SLSQP)\n","    opts = minimize(objective_f, x0=x_0, method='SLSQP', bounds=bnds, constraints=cons)\n","    # if opts['message'] != 'Optimization terminated successfully.':\n","    #     print(opts['message'])\n","    \n","    def numpy_unflatten(array, position):\n","        return np.expand_dims(array, axis=position)\n","\n","    return numpy_unflatten(opts['x'],1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_kilt_L7Z9x"},"outputs":[],"source":["def compute_cov(log_ret,FF_factors):\n","\n","    # print(\"=========calculating linear shrinkage versions=============\")\n","    linear_covar=[]\n","    linear_covar.append(cov1Para(log_ret))\n","    # print(linear_covar[0].shape)\n","    linear_covar.append(cov2Para(log_ret))\n","    # print(linear_covar[1].shape)\n","    linear_covar.append(covCor(log_ret))\n","    # print(linear_covar[2].shape)\n","    linear_covar.append(covDiag(log_ret))\n","    # print(linear_covar[3].shape)\n","    linear_covar.append(covMarket(log_ret))\n","    # print(linear_covar[4].shape)\n","    \n","    # print(\"=========calculating non-linear shrinkage versions=============\")\n","    non_linear_covar=[]\n","    non_linear_covar.append(LIS(log_ret))\n","    # print(non_linear_covar[0].shape)\n","    non_linear_covar.append(QIS(log_ret))\n","    # print(non_linear_covar[1].shape)\n","    non_linear_covar.append(GIS(log_ret))\n","    # print(non_linear_covar[2].shape)\n","    \n","    # print(\"=========calculating paper shrinkage versions=============\")\n","    \n","    \"\"\"\n","    1. equal_weight\n","    2. sample_cov\n","    3. lin_shrink\n","    4. non_lin_shrink\n","    5. single_factor\n","    6. FamaFrench\n","    7. NLSF\n","    8. POET\n","    \"\"\"\n","    other_covar = []\n","    other_covar.append(equal_weight(log_ret))\n","    # print(other_covar[0].shape)\n","    other_covar.append(sample_cov(log_ret))\n","    # print(other_covar[1].shape)\n","    other_covar.append(lin_shrink(log_ret))\n","    # print(other_covar[2].shape)\n","    other_covar.append(non_lin_shrink(log_ret))\n","    # print(other_covar[3].shape)\n","    other_covar.append(single_factor(log_ret))\n","    # print(other_covar[4].shape)\n","    other_covar.append(FamaFrench(log_ret,FF_factors))\n","    # print(other_covar[5].shape)\n","    other_covar.append(POET(log_ret))\n","    # print(other_covar[7].shape)\n","    other_covar.append(NLSF(log_ret))\n","    # print(other_covar[6].shape)\n","\n","    return linear_covar, non_linear_covar, other_covar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RQMYMZaktlW4","executionInfo":{"status":"ok","timestamp":1682947691775,"user_tz":-120,"elapsed":481,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"2688326b-df20-40af-d437-1f754c365f0c"},"outputs":[{"output_type":"stream","name":"stdout","text":["total_universes: 482\n","total_months given: 494.3809523809524\n","number of days required for 494 months:10374\n","CPU times: user 2.08 ms, sys: 0 ns, total: 2.08 ms\n","Wall time: 2.09 ms\n"]}],"source":["%%time \n","from tqdm import tqdm\n","#calcuate covariance matrices for each universe \n","month_trading_days = 21\n","year_trading_days = month_trading_days*12\n","total_months = returns.shape[0]/21\n","#total universes is months starting from 1 year after the beginning date\n","total_universes = int((returns.shape[0]-year_trading_days)/month_trading_days)\n","print(f\"total_universes: { total_universes}\")\n","print(f\"total_months given: { total_months}\")\n","print(f\"number of days required for 494 months:{21*494}\")\n","\n","SEED = 42 \n","import random\n","random.seed(SEED)\n","\n","def train_and_predict(N,learn_days,test_days,log_returns,k):\n","\n","    def flat_list(input_list):\n","      import itertools\n","      return list(itertools.chain.from_iterable(input_list))\n","\n","    \"\"\"\n","    define the lists that accumulate all the neccessary values\n","    \"\"\"\n","    #####Linear####### including short selling and not including short selling\n","    linearss1 = []\n","    linearss2 = []\n","    linearss3 = []\n","    linearss4 = []\n","    linearss5 = []\n","\n","    linearnss1 = []\n","    linearnss2 = []\n","    linearnss3 = []\n","    linearnss4 = []\n","    linearnss5 = []\n","\n","    #####Non - Linear####### including short selling and not including short selling\n","    non_linearss1 = []\n","    non_linearss2 = []\n","    non_linearss3 = []\n","    \n","    non_linearnss1 = []\n","    non_linearnss2 = []\n","    non_linearnss3 = []\n","    \n","    #####From Paper####### including short selling and not including short selling\n","    otherss1 = []\n","    otherss2 = []\n","    otherss3 = []\n","    otherss4 = []\n","    otherss5 = []\n","    otherss3 = []\n","    otherss4 = []\n","    otherss5 = []\n","    otherss6 = []\n","    otherss7 = []\n","    otherss8 = []\n","\n","    othernss1 = []\n","    othernss2 = []\n","    othernss3 = []\n","    othernss4 = []\n","    othernss5 = []\n","    othernss3 = []\n","    othernss4 = []\n","    othernss5 = []\n","    othernss6 = []\n","    othernss7 = []\n","    othernss8 = []\n","\n","    ############## weights dictionaries with time###############\n","    weights_lss1 = []\n","    weights_lss2 = []\n","    weights_lss3 = []\n","    weights_lss4 = []\n","    weights_lss5 = []\n","\n","    weights_lnss1 = []\n","    weights_lnss2 = []\n","    weights_lnss3 = []\n","    weights_lnss4 = []\n","    weights_lnss5 = []\n","\n","    ##non-linear###\n","    weigthts_nlss1 = []\n","    weigthts_nlss2 = []\n","    weigthts_nlss3 = []\n","\n","    weigthts_nlnss1 = []\n","    weigthts_nlnss2 = []\n","    weigthts_nlnss3 = []\n","\n","    #paper weights###\n","    weights_oss1 = []\n","    weights_oss2 = []\n","    weights_oss3 = []\n","    weights_oss4 = []\n","    weights_oss5 = []\n","    weights_oss6 = []\n","    weights_oss7 = []\n","    weights_oss8 = []\n","\n","    weights_onss1 = []\n","    weights_onss2 = []\n","    weights_onss3 = []\n","    weights_onss4 = []\n","    weights_onss5 = []\n","    weights_onss6 = []\n","    weights_onss7 = []\n","    weights_onss8 = []\n","\n","\n","    \"\"\"\n","    take \n","    N = assets we want to predict fot\n","    learn_days = how many days are considered to be sample for learning\n","    test_days = how many days we test our prediction (these are the window days, or in real life, how often do we restrategize)\n","    \"\"\"\n","    import pandas as pd\n","    import random\n","    print(f\"stocks_chosen:{N},\\n learning_days: {learn_days},\\n test_days: {test_days},\\n, out of total best:{k} \")\n","    linear_returns=[]\n","    non_linear_returns = []\n","    def get_random_tickers(df, row_index, N, K):\n","        \"\"\"\n","        Randomly select N stock tickers out of every K top-performing stocks based on a given row.\n","\n","        Args:\n","            df (pd.DataFrame): DataFrame with column names representing stock tickers.\n","            row_index (int): Index of the row to consider for selecting stocks.\n","            N (int): Number of stock tickers to select.\n","            K (int): Number of top-performing stocks to consider.\n","\n","        Returns:\n","            list: List of randomly selected stock tickers.\n","        \"\"\"\n","        # print(\"row_index:\",row_index)\n","        row_values = df.iloc[row_index]\n","        sorted_values = row_values.sort_values(ascending=False)\n","        top_k_tickers = sorted_values[:K].index.tolist()\n","        selected_tickers = random.sample(top_k_tickers, N)\n","        return selected_tickers\n","        \n","    universes = int((log_returns.shape[0]-learn_days)/test_days)\n","    for universe in tqdm(range(universes)):\n","        # print(\"universe:\", universe+1)\n","        tickers = get_random_tickers(log_returns.iloc[:,1:],learn_days+(universe+1)*test_days,N,k)\n","        \n","        in_sample = log_returns.loc[universe*test_days:((universe*test_days)+learn_days)-1,tickers].copy()\n","        \"\"\"\n","        ff data already is daily data so, no need to worry about it. do not do any unnecessary modifications\n","        \"\"\"\n","        ff_factors = ff_data.iloc[universe*test_days:((universe*test_days)+learn_days), 1:-1].copy()\n","\n","        out_of_sample = returns.loc[(universe*test_days)+learn_days:((universe+1)*test_days)+learn_days-1, tickers].copy()\n","        # print(\"out_of_sample mean\",len(out_of_sample.mean().values))\n","\n","        rf_data = ff_data.iloc[(universe*test_days)+learn_days:((universe+1)*test_days)+learn_days,-1].copy()\n","        rf_data = np.array(rf_data).T\n","        #print(rf_data.shape) #shape is 21,\n","\n","        # compute covariance using all the listed models\n","        linear_covar, non_linear_covar, paper_covar = compute_cov(in_sample,ff_factors)\n","        for i,linear in enumerate(linear_covar):\n","            weights_ss = get_weights_allow_ss(linear).T\n","            real_returns_ss = np.matmul(weights_ss,out_of_sample.values.T)\n","            weights_no_ss = get_weights_notallow_ss(out_of_sample.mean().values,linear).T\n","            real_returns_no_ss = np.matmul(weights_no_ss,out_of_sample.values.T)\n","            market_return_ss = flat_list((real_returns_ss-rf_data).tolist())\n","            market_retuns_nss = flat_list((real_returns_no_ss-rf_data).tolist())\n","            weights_ss_dict = dict(zip(tickers,np.array(weights_ss).flatten().tolist()))\n","            weights_nss_dict = dict(zip(tickers,np.array(weights_no_ss).flatten().tolist()))\n","\n","            #calculate and append returns and weight dictionaries for each iteration\n","            if i+1 == 1:\n","                linearss1+=market_return_ss\n","                linearnss1+=market_retuns_nss\n","                weights_lss1.append(weights_ss_dict)\n","                weights_lnss1.append(weights_nss_dict)\n","\n","            if i+1 == 2:\n","                linearss2+=market_return_ss\n","                linearnss2+=market_retuns_nss\n","                weights_lss2.append(weights_ss_dict)\n","                weights_lnss2.append(weights_nss_dict)\n","\n","            if i+1 == 3:\n","                linearss3+=market_return_ss\n","                linearnss3+=market_retuns_nss\n","                weights_lss3.append(weights_ss_dict)\n","                weights_lnss3.append(weights_nss_dict)\n","            \n","            if i+1 == 4:\n","                linearss4+=market_return_ss\n","                linearnss4+=market_retuns_nss\n","                weights_lss4.append(weights_ss_dict)\n","                weights_lnss4.append(weights_nss_dict)\n","\n","            if i+1 == 5:\n","                linearss5+=market_return_ss\n","                linearnss5+=market_retuns_nss\n","                weights_lss5.append(weights_ss_dict)\n","                weights_lnss5.append(weights_nss_dict)\n","\n","        for i,non_lin in enumerate(non_linear_covar):\n","            weights_ss = get_weights_allow_ss(non_lin).T\n","            real_returns_ss = np.matmul(weights_ss,out_of_sample.values.T)\n","            weights_no_ss = get_weights_notallow_ss(out_of_sample.mean().values,non_lin).T\n","            real_returns_no_ss = np.matmul(weights_no_ss,out_of_sample.values.T)\n","            market_return_ss = flat_list((real_returns_ss-rf_data).tolist())\n","            market_retuns_nss = flat_list((real_returns_no_ss-rf_data).tolist())\n","            weights_ss_dict = dict(zip(tickers,np.array(weights_ss).flatten().tolist()))\n","            weights_nss_dict = dict(zip(tickers,np.array(weights_no_ss).flatten().tolist()))\n","            \n","            if i+1 == 1:\n","                non_linearss1+=market_return_ss\n","                non_linearnss1+=market_retuns_nss\n","                weigthts_nlss1.append(weights_ss_dict)\n","                weigthts_nlnss1.append(weights_nss_dict)\n","\n","            if i+1 == 2:\n","                non_linearss2+=market_return_ss\n","                non_linearnss2+=market_retuns_nss\n","                weigthts_nlss2.append(weights_ss_dict)\n","                weigthts_nlnss2.append(weights_nss_dict)\n","\n","            if i+1 == 3:\n","                non_linearss3+=market_return_ss\n","                non_linearnss3+=market_retuns_nss\n","                weigthts_nlss3.append(weights_ss_dict)\n","                weigthts_nlnss3.append(weights_nss_dict) \n","\n","        for i,other in enumerate(paper_covar):\n","            weights_ss = get_weights_allow_ss(other).T\n","            real_returns_ss = np.matmul(weights_ss,out_of_sample.values.T)\n","            weights_no_ss = get_weights_notallow_ss(out_of_sample.mean().values,other).T\n","            real_returns_no_ss = np.matmul(weights_no_ss,out_of_sample.values.T)\n","            market_return_ss = flat_list((real_returns_ss-rf_data).tolist())\n","            market_retuns_nss = flat_list((real_returns_no_ss-rf_data).tolist())\n","            weights_ss_dict = dict(zip(tickers,np.array(weights_ss).flatten().tolist()))\n","            weights_nss_dict = dict(zip(tickers,np.array(weights_no_ss).flatten().tolist()))\n","            #calculate and append returns and weight dictionaries for each iteration\n","            if i+1 == 1:\n","                otherss1+=market_return_ss\n","                othernss1+=market_retuns_nss\n","                weights_oss1.append(weights_ss_dict)\n","                weights_onss1.append(weights_nss_dict)\n","\n","            if i+1 == 2:\n","                otherss2+=market_return_ss\n","                othernss2+=market_retuns_nss\n","                weights_oss2.append(weights_ss_dict)\n","                weights_onss2.append(weights_nss_dict)\n","\n","            if i+1 == 3:\n","                otherss3+=market_return_ss\n","                othernss3+=market_retuns_nss\n","                weights_oss3.append(weights_ss_dict)\n","                weights_onss3.append(weights_nss_dict)\n","            \n","            if i+1 == 4:\n","                otherss4+=market_return_ss\n","                othernss4+=market_retuns_nss\n","                weights_oss4.append(weights_ss_dict)\n","                weights_onss4.append(weights_nss_dict)\n","\n","            if i+1 == 5:\n","                otherss5+=market_return_ss\n","                othernss5+=market_retuns_nss\n","                weights_oss5.append(weights_ss_dict)\n","                weights_onss5.append(weights_nss_dict)\n","            \n","            if i+1 == 6:\n","                otherss6+=market_return_ss\n","                othernss6+=market_retuns_nss\n","                weights_oss6.append(weights_ss_dict)\n","                weights_onss6.append(weights_nss_dict)\n","            \n","            if i+1 == 7:\n","                otherss7+=market_return_ss\n","                othernss7+=market_retuns_nss\n","                weights_oss7.append(weights_ss_dict)\n","                weights_onss7.append(weights_nss_dict)\n","\n","            if i+1 == 8:\n","                otherss8+=market_return_ss\n","                othernss8+=market_retuns_nss\n","                weights_oss8.append(weights_ss_dict)\n","                weights_onss8.append(weights_nss_dict)\n","    \n","    #create dataframes and return for the sample\n","    # print(len(linearss1),  len(linearss2),  len(linearss3),len(linearss4), len(linearss5), len(non_linearss1), len(non_linearss2),\n","    #       len(non_linearss3),len(otherss1),len(otherss2),len(otherss3),len(otherss4),len(otherss5),\n","    #       len(otherss6),len(otherss7),len(otherss8))\n","\n","    return_columns = [\"cov1Para\",\"cov2Para\",\"covCor\",\"covDiag\",\"covMarket\",\"LIS\",\"QIS\",\"GIS\",\"equal_weight\",\"sample_cov\",\"lin_shrink\",\"non_lin_shrink\",\"single_factor\",\"FamaFrench\",\"POET\",\"NLSF\"]\n","    ss_dummy = np.stack((linearss1,\n","              linearss2,\n","              linearss3,\n","              linearss4,\n","              linearss5,\n","              non_linearss1,\n","              non_linearss2,\n","              non_linearss3,\n","              otherss1,\n","              otherss2,\n","              otherss3,\n","              otherss4,\n","              otherss5,\n","              otherss6,\n","              otherss7,\n","              otherss8)).T\n","    \n","    nss_dummy = np.stack((linearnss1,\n","              linearnss2,\n","              linearnss3,\n","              linearnss4,\n","              linearnss5,\n","              non_linearnss1,\n","              non_linearnss2,\n","              non_linearnss3,\n","              othernss1,\n","              othernss2,\n","              othernss3,\n","              othernss4,\n","              othernss5,\n","              othernss6,\n","              othernss7,\n","              othernss8)).T\n","\n","    ss_weights_dummy = [weights_lss1,\n","                        weights_lss2,\n","                        weights_lss3,\n","                        weights_lss4,\n","                        weights_lss5,\n","                        weigthts_nlss1,\n","                        weigthts_nlss2,\n","                        weigthts_nlss3,\n","                        weights_oss1,\n","                        weights_oss2,\n","                        weights_oss3,\n","                        weights_oss4,\n","                        weights_oss5,\n","                        weights_oss6,\n","                        weights_oss7,\n","                        weights_oss8]\n","\n","    nss_weights_dummy = [weights_lnss1,\n","                          weights_lnss2,\n","                          weights_lnss3,\n","                          weights_lnss4,\n","                          weights_lnss5,\n","                          weigthts_nlnss1,\n","                          weigthts_nlnss2,\n","                          weigthts_nlnss3,\n","                          weights_onss1,\n","                          weights_onss2,\n","                          weights_onss3,\n","                          weights_onss4,\n","                          weights_onss5,\n","                          weights_onss6,\n","                          weights_onss7,\n","                          weights_onss8]\n","\n","    ss_df = pd.DataFrame(ss_dummy,columns = return_columns)\n","    # print(ss_df.head())\n","    nss_df = pd.DataFrame(nss_dummy,columns = return_columns)\n","    # print(nss_df.head())\n","\n","    ss_weights_df = pd.DataFrame(ss_weights_dummy).T\n","    ss_weights_df.columns = return_columns\n","\n","    nss_weights_df = pd.DataFrame(nss_weights_dummy).T\n","    nss_weights_df.columns = return_columns\n","\n","    return ss_df, nss_df, ss_weights_df, nss_weights_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XJWFYPaAHvHZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682951711161,"user_tz":-120,"elapsed":4015856,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"164b63c0-56d3-4d9d-f550-9f3a3ac021a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["stocks_chosen:100,\n"," learning_days: 252,\n"," test_days: 21,\n",", out of total best:150 \n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 482/482 [46:41<00:00,  5.81s/it]\n"]},{"output_type":"stream","name":"stdout","text":["stocks_chosen:50,\n"," learning_days: 252,\n"," test_days: 21,\n",", out of total best:150 \n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 482/482 [13:03<00:00,  1.62s/it]\n"]},{"output_type":"stream","name":"stdout","text":["stocks_chosen:30,\n"," learning_days: 252,\n"," test_days: 21,\n",", out of total best:150 \n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 482/482 [07:11<00:00,  1.12it/s]\n"]}],"source":["#here I am passing returns data which is percentage data from the earlier population direclt\n","\n","hund_ss_df, hund_nss_df, hund_ss_weights_df, hund_nss_weights_df = train_and_predict(100,year_trading_days,month_trading_days,returns,150)\n","fifty_ss_df, fifty_nss_df, fifty_ss_weights_df, fifty_nss_weights_df = train_and_predict(50,year_trading_days,month_trading_days,returns,150)\n","thirty_ss_df, thirty_nss_df, thirty_ss_weights_df, thirty_nss_weights_df = train_and_predict(30,year_trading_days,month_trading_days,returns,150)"]},{"cell_type":"code","source":["def plot_return_values(df, columns, dates = pd.Series(returns[\"date\"][252:252+hund_ss_df.shape[0]].values)):\n","    \n","    import matplotlib.pyplot as plt\n","    import warnings\n","    warnings.filterwarnings(\"ignore\")\n","    \n","    df = df[columns]\n","    df[\"date\"]= dates.apply(convert_to_date)\n","\n","    df['date'] = pd.to_datetime(df['date'])\n","    df.set_index('date', inplace=True)\n","\n","    # Create a color palette with distinct colors\n","    color_palette = plt.cm.Set1.colors[:df.shape[1]]\n","\n","    # Plot columns with distinct colors and adjust the line width\n","    df.plot(color=color_palette, lw=2)\n","\n","    plt.xlabel('Date')\n","    plt.ylabel('Value')\n","    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n","\n","    plt.tight_layout()\n","\n","    plt.show()"],"metadata":{"id":"apcXce3g3GEw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -> choose out of these columns\n","#\"cov1Para\",\"cov2Para\",\"covCor\",\"covDiag\",\"covMarket\",\"LIS\",\"QIS\",\"GIS\",\"equal_weight\",\"sample_cov\",\"lin_shrink\",\"lin_shrink\",\"single_factor\",\"FamaFrench\",\"POET\",\"NLSF\"]]\n","plot_columns = [\"equal_weight\",\"sample_cov\",\"lin_shrink\",\"non_lin_shrink\"]\n","\n","#function to plot return values\n","\"\"\"\n","1.takes the dataframe\n","2.takes the columns (here the strategies used), \n","3. if you want to give different dates, you can but not needed as long as you are plotting for all the days - 10112\n","\"\"\"\n","plot_return_values(hund_ss_df,plot_columns)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":487},"id":"V6tTeA2D3pvQ","executionInfo":{"status":"ok","timestamp":1682954016302,"user_tz":-120,"elapsed":1543,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"a5234bb6-37eb-4dad-a8c0-d482eb3a0a32"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["#get the final returns of all the values per columns\n","def get_avg_returns(df,days= 252):\n","    \"\"\"\n","    this function takes dataframe and puts together the annual return overall of each column\n","    \"\"\"\n","    mean_df = df.mean()\n","    mean_df.columns = df.columns\n","    mean_df = mean_df*days\n","    return mean_df\n","\n","def get_stdev(df,days= 252):\n","    std_df = df.std()*(days**(0.5))\n","    return std_df"],"metadata":{"id":"mVOHgZkewg-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#all the dataframe\n","all_dfs = [\"hund_ss_df\", \"hund_nss_df\", \"fifty_ss_df\", \"fifty_nss_df\",\"thirty_ss_df\", \"thirty_nss_df\"]\n","\n","#put all data into single dataframe\n","average_all_dfs = pd.DataFrame()\n","average_all_dfs = pd.concat([average_all_dfs,get_avg_returns(hund_ss_df).to_frame()],axis=1)\n","average_all_dfs = pd.concat([average_all_dfs,get_avg_returns(hund_nss_df).to_frame()],axis=1)\n","average_all_dfs = pd.concat([average_all_dfs,get_avg_returns(fifty_ss_df).to_frame()],axis=1)\n","average_all_dfs = pd.concat([average_all_dfs,get_avg_returns(fifty_nss_df).to_frame()],axis=1)\n","average_all_dfs = pd.concat([average_all_dfs,get_avg_returns(thirty_ss_df).to_frame()],axis=1)\n","average_all_dfs = pd.concat([average_all_dfs,get_avg_returns(thirty_nss_df).to_frame()],axis=1)\n","\n","average_all_dfs.columns = all_dfs\n","\n","display(average_all_dfs)\n","display(pd.concat([average_all_dfs.idxmax(), average_all_dfs.max()],axis=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":772},"id":"0gsVGdRAIvFe","executionInfo":{"status":"ok","timestamp":1682977430416,"user_tz":-120,"elapsed":311,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"d16a63c2-5749-4934-da0f-360842de3e53"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["                hund_ss_df  hund_nss_df  fifty_ss_df  fifty_nss_df  \\\n","cov1Para         -3.212151    -1.769701    -3.222992     -1.902201   \n","cov2Para         -3.210608    -1.631455    -3.221167     -1.804637   \n","covCor           -3.229437    -1.705674    -3.233650     -1.860737   \n","covDiag          -3.216571    -1.796164    -3.226604     -1.928460   \n","covMarket        -3.222368    -1.775579    -3.232848     -1.900143   \n","LIS              -3.217282    -1.756310    -3.228816     -1.894051   \n","QIS              -3.217259    -1.747178    -3.228919     -1.890232   \n","GIS              -3.217276    -1.751755    -3.228866     -1.892171   \n","equal_weight     -3.175017    -2.226032    -3.175282     -2.235917   \n","sample_cov       -3.216488    -1.779975    -3.229384     -1.911421   \n","lin_shrink       -3.212151    -1.769706    -3.222991     -1.902191   \n","non_lin_shrink   -3.217357    -1.741482    -3.228642     -1.887094   \n","single_factor    -3.191133    -2.382132    -3.194284     -2.403414   \n","FamaFrench       -3.226494    -1.825446    -3.232788     -1.938909   \n","POET             -3.223038    -2.184888    -3.219356     -2.269304   \n","NLSF             -3.219895    -1.775690    -3.227511     -1.906663   \n","\n","                thirty_ss_df  thirty_nss_df  \n","cov1Para           -3.194976      -2.043717  \n","cov2Para           -3.195722      -1.979195  \n","covCor             -3.193991      -2.020136  \n","covDiag            -3.192619      -2.064246  \n","covMarket          -3.196385      -2.041085  \n","LIS                -3.197214      -2.040091  \n","QIS                -3.197314      -2.038586  \n","GIS                -3.197264      -2.039349  \n","equal_weight       -3.171963      -2.294425  \n","sample_cov         -3.193986      -2.047976  \n","lin_shrink         -3.194977      -2.043748  \n","non_lin_shrink     -3.197793      -2.036690  \n","single_factor      -3.181788      -2.427832  \n","FamaFrench         -3.197970      -2.070069  \n","POET               -3.202463      -2.352729  \n","NLSF               -3.195230      -2.051409  "],"text/html":["\n","  <div id=\"df-f7973d93-c912-460a-b621-6e37808fadd6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hund_ss_df</th>\n","      <th>hund_nss_df</th>\n","      <th>fifty_ss_df</th>\n","      <th>fifty_nss_df</th>\n","      <th>thirty_ss_df</th>\n","      <th>thirty_nss_df</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>cov1Para</th>\n","      <td>-3.212151</td>\n","      <td>-1.769701</td>\n","      <td>-3.222992</td>\n","      <td>-1.902201</td>\n","      <td>-3.194976</td>\n","      <td>-2.043717</td>\n","    </tr>\n","    <tr>\n","      <th>cov2Para</th>\n","      <td>-3.210608</td>\n","      <td>-1.631455</td>\n","      <td>-3.221167</td>\n","      <td>-1.804637</td>\n","      <td>-3.195722</td>\n","      <td>-1.979195</td>\n","    </tr>\n","    <tr>\n","      <th>covCor</th>\n","      <td>-3.229437</td>\n","      <td>-1.705674</td>\n","      <td>-3.233650</td>\n","      <td>-1.860737</td>\n","      <td>-3.193991</td>\n","      <td>-2.020136</td>\n","    </tr>\n","    <tr>\n","      <th>covDiag</th>\n","      <td>-3.216571</td>\n","      <td>-1.796164</td>\n","      <td>-3.226604</td>\n","      <td>-1.928460</td>\n","      <td>-3.192619</td>\n","      <td>-2.064246</td>\n","    </tr>\n","    <tr>\n","      <th>covMarket</th>\n","      <td>-3.222368</td>\n","      <td>-1.775579</td>\n","      <td>-3.232848</td>\n","      <td>-1.900143</td>\n","      <td>-3.196385</td>\n","      <td>-2.041085</td>\n","    </tr>\n","    <tr>\n","      <th>LIS</th>\n","      <td>-3.217282</td>\n","      <td>-1.756310</td>\n","      <td>-3.228816</td>\n","      <td>-1.894051</td>\n","      <td>-3.197214</td>\n","      <td>-2.040091</td>\n","    </tr>\n","    <tr>\n","      <th>QIS</th>\n","      <td>-3.217259</td>\n","      <td>-1.747178</td>\n","      <td>-3.228919</td>\n","      <td>-1.890232</td>\n","      <td>-3.197314</td>\n","      <td>-2.038586</td>\n","    </tr>\n","    <tr>\n","      <th>GIS</th>\n","      <td>-3.217276</td>\n","      <td>-1.751755</td>\n","      <td>-3.228866</td>\n","      <td>-1.892171</td>\n","      <td>-3.197264</td>\n","      <td>-2.039349</td>\n","    </tr>\n","    <tr>\n","      <th>equal_weight</th>\n","      <td>-3.175017</td>\n","      <td>-2.226032</td>\n","      <td>-3.175282</td>\n","      <td>-2.235917</td>\n","      <td>-3.171963</td>\n","      <td>-2.294425</td>\n","    </tr>\n","    <tr>\n","      <th>sample_cov</th>\n","      <td>-3.216488</td>\n","      <td>-1.779975</td>\n","      <td>-3.229384</td>\n","      <td>-1.911421</td>\n","      <td>-3.193986</td>\n","      <td>-2.047976</td>\n","    </tr>\n","    <tr>\n","      <th>lin_shrink</th>\n","      <td>-3.212151</td>\n","      <td>-1.769706</td>\n","      <td>-3.222991</td>\n","      <td>-1.902191</td>\n","      <td>-3.194977</td>\n","      <td>-2.043748</td>\n","    </tr>\n","    <tr>\n","      <th>non_lin_shrink</th>\n","      <td>-3.217357</td>\n","      <td>-1.741482</td>\n","      <td>-3.228642</td>\n","      <td>-1.887094</td>\n","      <td>-3.197793</td>\n","      <td>-2.036690</td>\n","    </tr>\n","    <tr>\n","      <th>single_factor</th>\n","      <td>-3.191133</td>\n","      <td>-2.382132</td>\n","      <td>-3.194284</td>\n","      <td>-2.403414</td>\n","      <td>-3.181788</td>\n","      <td>-2.427832</td>\n","    </tr>\n","    <tr>\n","      <th>FamaFrench</th>\n","      <td>-3.226494</td>\n","      <td>-1.825446</td>\n","      <td>-3.232788</td>\n","      <td>-1.938909</td>\n","      <td>-3.197970</td>\n","      <td>-2.070069</td>\n","    </tr>\n","    <tr>\n","      <th>POET</th>\n","      <td>-3.223038</td>\n","      <td>-2.184888</td>\n","      <td>-3.219356</td>\n","      <td>-2.269304</td>\n","      <td>-3.202463</td>\n","      <td>-2.352729</td>\n","    </tr>\n","    <tr>\n","      <th>NLSF</th>\n","      <td>-3.219895</td>\n","      <td>-1.775690</td>\n","      <td>-3.227511</td>\n","      <td>-1.906663</td>\n","      <td>-3.195230</td>\n","      <td>-2.051409</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7973d93-c912-460a-b621-6e37808fadd6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f7973d93-c912-460a-b621-6e37808fadd6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f7973d93-c912-460a-b621-6e37808fadd6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["                          0         1\n","hund_ss_df     equal_weight -3.175017\n","hund_nss_df        cov2Para -1.631455\n","fifty_ss_df    equal_weight -3.175282\n","fifty_nss_df       cov2Para -1.804637\n","thirty_ss_df   equal_weight -3.171963\n","thirty_nss_df      cov2Para -1.979195"],"text/html":["\n","  <div id=\"df-21ad1da2-c76d-4ef2-b9da-ffee8f317242\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>hund_ss_df</th>\n","      <td>equal_weight</td>\n","      <td>-3.175017</td>\n","    </tr>\n","    <tr>\n","      <th>hund_nss_df</th>\n","      <td>cov2Para</td>\n","      <td>-1.631455</td>\n","    </tr>\n","    <tr>\n","      <th>fifty_ss_df</th>\n","      <td>equal_weight</td>\n","      <td>-3.175282</td>\n","    </tr>\n","    <tr>\n","      <th>fifty_nss_df</th>\n","      <td>cov2Para</td>\n","      <td>-1.804637</td>\n","    </tr>\n","    <tr>\n","      <th>thirty_ss_df</th>\n","      <td>equal_weight</td>\n","      <td>-3.171963</td>\n","    </tr>\n","    <tr>\n","      <th>thirty_nss_df</th>\n","      <td>cov2Para</td>\n","      <td>-1.979195</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-21ad1da2-c76d-4ef2-b9da-ffee8f317242')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-21ad1da2-c76d-4ef2-b9da-ffee8f317242 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-21ad1da2-c76d-4ef2-b9da-ffee8f317242');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["#all the dataframe\n","all_dfs = [\"hund_ss_df\", \"hund_nss_df\", \"fifty_ss_df\", \"fifty_nss_df\",\"thirty_ss_df\", \"thirty_nss_df\"]\n","\n","#put all data into single dataframe\n","stdev_all_dfs = pd.DataFrame()\n","stdev_all_dfs = pd.concat([stdev_all_dfs,get_stdev(hund_ss_df).to_frame()],axis=1)\n","stdev_all_dfs = pd.concat([stdev_all_dfs,get_stdev(hund_nss_df).to_frame()],axis=1)\n","stdev_all_dfs = pd.concat([stdev_all_dfs,get_stdev(fifty_ss_df).to_frame()],axis=1)\n","stdev_all_dfs = pd.concat([stdev_all_dfs,get_stdev(fifty_nss_df).to_frame()],axis=1)\n","stdev_all_dfs = pd.concat([stdev_all_dfs,get_stdev(thirty_ss_df).to_frame()],axis=1)\n","stdev_all_dfs = pd.concat([stdev_all_dfs,get_stdev(thirty_nss_df).to_frame()],axis=1)\n","\n","stdev_all_dfs.columns = all_dfs\n","\n","display(stdev_all_dfs)\n","display(pd.concat([stdev_all_dfs.idxmin(), stdev_all_dfs.min()],axis=1))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":772},"id":"AgvOMkQ_LLiA","executionInfo":{"status":"ok","timestamp":1682959758404,"user_tz":-120,"elapsed":439,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"f711e3e4-a26c-4aa5-9846-f61690748111"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["                hund_ss_df  hund_nss_df  fifty_ss_df  fifty_nss_df  \\\n","cov1Para          0.210487     0.315856     0.214879      0.307583   \n","cov2Para          0.209776     0.329713     0.214511      0.320598   \n","covCor            0.209782     0.325483     0.214855      0.316699   \n","covDiag           0.211207     0.313965     0.215570      0.304824   \n","covMarket         0.208485     0.318048     0.213842      0.309483   \n","LIS               0.209449     0.317774     0.214402      0.310137   \n","QIS               0.209336     0.319175     0.214356      0.310867   \n","GIS               0.209380     0.318456     0.214376      0.310449   \n","equal_weight      0.243772     0.273443     0.245543      0.280860   \n","sample_cov        0.217248     0.316982     0.217878      0.308354   \n","lin_shrink        0.210486     0.315853     0.214878      0.307586   \n","non_lin_shrink    0.209423     0.320114     0.214312      0.311306   \n","single_factor     0.228857     0.247435     0.230616      0.257838   \n","FamaFrench        0.213873     0.313229     0.217103      0.302801   \n","POET              0.212026     0.279672     0.216803      0.270559   \n","NLSF              0.208487     0.316116     0.213768      0.309298   \n","\n","                thirty_ss_df  thirty_nss_df  \n","cov1Para            0.218166       0.296329  \n","cov2Para            0.217865       0.304633  \n","covCor              0.218101       0.300753  \n","covDiag             0.218286       0.295149  \n","covMarket           0.217684       0.298590  \n","LIS                 0.218192       0.297818  \n","QIS                 0.218203       0.298106  \n","GIS                 0.218197       0.297955  \n","equal_weight        0.246180       0.273572  \n","sample_cov          0.220119       0.297882  \n","lin_shrink          0.218166       0.296314  \n","non_lin_shrink      0.218218       0.298490  \n","single_factor       0.230760       0.258894  \n","FamaFrench          0.219038       0.295511  \n","POET                0.219837       0.260852  \n","NLSF                0.217768       0.297652  "],"text/html":["\n","  <div id=\"df-5fcad090-4b61-4bd6-a7e3-6df631a2bf65\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>hund_ss_df</th>\n","      <th>hund_nss_df</th>\n","      <th>fifty_ss_df</th>\n","      <th>fifty_nss_df</th>\n","      <th>thirty_ss_df</th>\n","      <th>thirty_nss_df</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>cov1Para</th>\n","      <td>0.210487</td>\n","      <td>0.315856</td>\n","      <td>0.214879</td>\n","      <td>0.307583</td>\n","      <td>0.218166</td>\n","      <td>0.296329</td>\n","    </tr>\n","    <tr>\n","      <th>cov2Para</th>\n","      <td>0.209776</td>\n","      <td>0.329713</td>\n","      <td>0.214511</td>\n","      <td>0.320598</td>\n","      <td>0.217865</td>\n","      <td>0.304633</td>\n","    </tr>\n","    <tr>\n","      <th>covCor</th>\n","      <td>0.209782</td>\n","      <td>0.325483</td>\n","      <td>0.214855</td>\n","      <td>0.316699</td>\n","      <td>0.218101</td>\n","      <td>0.300753</td>\n","    </tr>\n","    <tr>\n","      <th>covDiag</th>\n","      <td>0.211207</td>\n","      <td>0.313965</td>\n","      <td>0.215570</td>\n","      <td>0.304824</td>\n","      <td>0.218286</td>\n","      <td>0.295149</td>\n","    </tr>\n","    <tr>\n","      <th>covMarket</th>\n","      <td>0.208485</td>\n","      <td>0.318048</td>\n","      <td>0.213842</td>\n","      <td>0.309483</td>\n","      <td>0.217684</td>\n","      <td>0.298590</td>\n","    </tr>\n","    <tr>\n","      <th>LIS</th>\n","      <td>0.209449</td>\n","      <td>0.317774</td>\n","      <td>0.214402</td>\n","      <td>0.310137</td>\n","      <td>0.218192</td>\n","      <td>0.297818</td>\n","    </tr>\n","    <tr>\n","      <th>QIS</th>\n","      <td>0.209336</td>\n","      <td>0.319175</td>\n","      <td>0.214356</td>\n","      <td>0.310867</td>\n","      <td>0.218203</td>\n","      <td>0.298106</td>\n","    </tr>\n","    <tr>\n","      <th>GIS</th>\n","      <td>0.209380</td>\n","      <td>0.318456</td>\n","      <td>0.214376</td>\n","      <td>0.310449</td>\n","      <td>0.218197</td>\n","      <td>0.297955</td>\n","    </tr>\n","    <tr>\n","      <th>equal_weight</th>\n","      <td>0.243772</td>\n","      <td>0.273443</td>\n","      <td>0.245543</td>\n","      <td>0.280860</td>\n","      <td>0.246180</td>\n","      <td>0.273572</td>\n","    </tr>\n","    <tr>\n","      <th>sample_cov</th>\n","      <td>0.217248</td>\n","      <td>0.316982</td>\n","      <td>0.217878</td>\n","      <td>0.308354</td>\n","      <td>0.220119</td>\n","      <td>0.297882</td>\n","    </tr>\n","    <tr>\n","      <th>lin_shrink</th>\n","      <td>0.210486</td>\n","      <td>0.315853</td>\n","      <td>0.214878</td>\n","      <td>0.307586</td>\n","      <td>0.218166</td>\n","      <td>0.296314</td>\n","    </tr>\n","    <tr>\n","      <th>non_lin_shrink</th>\n","      <td>0.209423</td>\n","      <td>0.320114</td>\n","      <td>0.214312</td>\n","      <td>0.311306</td>\n","      <td>0.218218</td>\n","      <td>0.298490</td>\n","    </tr>\n","    <tr>\n","      <th>single_factor</th>\n","      <td>0.228857</td>\n","      <td>0.247435</td>\n","      <td>0.230616</td>\n","      <td>0.257838</td>\n","      <td>0.230760</td>\n","      <td>0.258894</td>\n","    </tr>\n","    <tr>\n","      <th>FamaFrench</th>\n","      <td>0.213873</td>\n","      <td>0.313229</td>\n","      <td>0.217103</td>\n","      <td>0.302801</td>\n","      <td>0.219038</td>\n","      <td>0.295511</td>\n","    </tr>\n","    <tr>\n","      <th>POET</th>\n","      <td>0.212026</td>\n","      <td>0.279672</td>\n","      <td>0.216803</td>\n","      <td>0.270559</td>\n","      <td>0.219837</td>\n","      <td>0.260852</td>\n","    </tr>\n","    <tr>\n","      <th>NLSF</th>\n","      <td>0.208487</td>\n","      <td>0.316116</td>\n","      <td>0.213768</td>\n","      <td>0.309298</td>\n","      <td>0.217768</td>\n","      <td>0.297652</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5fcad090-4b61-4bd6-a7e3-6df631a2bf65')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5fcad090-4b61-4bd6-a7e3-6df631a2bf65 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5fcad090-4b61-4bd6-a7e3-6df631a2bf65');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["                           0         1\n","hund_ss_df         covMarket  0.208485\n","hund_nss_df    single_factor  0.247435\n","fifty_ss_df             NLSF  0.213768\n","fifty_nss_df   single_factor  0.257838\n","thirty_ss_df       covMarket  0.217684\n","thirty_nss_df  single_factor  0.258894"],"text/html":["\n","  <div id=\"df-a1106820-a7fc-4cd9-b8e0-666d270edb3e\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>hund_ss_df</th>\n","      <td>covMarket</td>\n","      <td>0.208485</td>\n","    </tr>\n","    <tr>\n","      <th>hund_nss_df</th>\n","      <td>single_factor</td>\n","      <td>0.247435</td>\n","    </tr>\n","    <tr>\n","      <th>fifty_ss_df</th>\n","      <td>NLSF</td>\n","      <td>0.213768</td>\n","    </tr>\n","    <tr>\n","      <th>fifty_nss_df</th>\n","      <td>single_factor</td>\n","      <td>0.257838</td>\n","    </tr>\n","    <tr>\n","      <th>thirty_ss_df</th>\n","      <td>covMarket</td>\n","      <td>0.217684</td>\n","    </tr>\n","    <tr>\n","      <th>thirty_nss_df</th>\n","      <td>single_factor</td>\n","      <td>0.258894</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1106820-a7fc-4cd9-b8e0-666d270edb3e')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a1106820-a7fc-4cd9-b8e0-666d270edb3e button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a1106820-a7fc-4cd9-b8e0-666d270edb3e');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}]},{"cell_type":"code","source":["import pandas as pd\n","import scipy.stats as stats\n","\n","def hypothesis_test(df, target_col, compare_cols):\n","    target_std = df[target_col].std()\n","    compare_stds = [df[col].std() for col in compare_cols]\n","\n","    # Perform one-sample t-test comparing the target column's standard deviation against the other columns\n","    _, p_value = stats.ttest_1samp(compare_stds, target_std)\n","\n","    # Print results\n","    print(\"Null Hypothesis: The standard deviation of column '{}' is not significantly lower than other columns\".format(target_col))\n","    print(\"Alternative Hypothesis: The standard deviation of column '{}' is significantly lower than other columns\".format(target_col))\n","    print(\"\")\n","\n","    if p_value < 0.05:\n","        print(\"Result: Reject the null hypothesis\")\n","        print(\"Conclusion: The standard deviation of column '{}' is significantly lower than other columns\".format(target_col))\n","    else:\n","        print(\"Result: Fail to reject the null hypothesis\")\n","        print(\"Conclusion: The standard deviation of column '{}' is not significantly lower than other columns\".format(target_col))\n","    print(\"p-value: \", p_value)\n","\n","hypothesis_column = \"non_lin_shrink\"\n","comparison_columns = [\"cov1Para\",\"cov2Para\"]\n","\n","hypothesis_test(hund_ss_df,hypothesis_column,comparison_columns )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-D_06b0396MM","executionInfo":{"status":"ok","timestamp":1682956147408,"user_tz":-120,"elapsed":270,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"b3b92322-3ab2-4587-8c19-664578f12a12"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Null Hypothesis: The standard deviation of column 'non_lin_shrink' is not significantly lower than other columns\n","Alternative Hypothesis: The standard deviation of column 'non_lin_shrink' is significantly lower than other columns\n","\n","Result: Fail to reject the null hypothesis\n","Conclusion: The standard deviation of column 'non_lin_shrink' is not significantly lower than other columns\n","p-value:  0.2959048935139657\n"]}]},{"cell_type":"markdown","metadata":{"id":"wmU4gD9fFE_f"},"source":["## **Ledoit Wolf Provided Functions**\n","- 2003 - Flexible multivariate GARCH modelling with an application to international stock markets\n","- 2003 - Improved estimation of covariance matrix of stock returns with an applicaiton in portfolio selection\n","- 2004 - A well conditioned estimator for large dimensional covariance matrices\n","- 2004 - Honey, I shrunk sample covariance matrix\n","- 2019 - Large Dynamic Covariance Matrices\n","- 2020 - Analytical non-linear shrinkage of large dimensional covariance matrices \n","- 2022 - Quadratic shrinkage for large covariance matrices"]},{"cell_type":"markdown","metadata":{"id":"VMs_aeGhGVjT"},"source":["### implementations from ledoit and wolf in python available"]},{"cell_type":"markdown","metadata":{"id":"p_BCcEdnIuo8"},"source":["> INPUT(S): Y (N*p): raw data matrix of N iid observations on p random variables. Second optional input parameter: If the second (optional) parameter k is absent, not-a-number, or empty,then the algorithm demeans the data by default, and adjusts the effective sample size accordingly. If the user inputs k = 0, then no demeaning takes place; if user inputs k = 1, then it signifies that the data Y has already been demeaned.\n","\n","Correction: In the term of matrices: \n","The input matrix should be of format (T time stamp rows * N stocks as columns)\n","Here N defined above is N time period and p as stocks."]},{"cell_type":"markdown","metadata":{"id":"oXB-iv00IWtv"},"source":["NONLINEAR SHRINKAGE: \n","\n","\n","> 6) GIS.m: Nonlinear shrinkage derived under the Symmetrized Kullback-Leibler loss, called geometric-inverse shrinkage (GIS). It can be viewed as geometrically averaging linear-inverse shrinkage (LIS) with quadratic-inverse shrinkage (QIS). See Ledoit and Wolf (2021, Remark 4.3).\n","\n","\n","> 7) LIS.m: Nonlinear shrinkage derived under Stein’s loss, called linear-inverse shrinkage (LIS). See Ledoit and Wolf (2021, Section 3). \n","8) QIS.m: Nonlinear shrinkage derived under Frobenius loss and its two cousins, Inverse Stein’s loss and Minimum Variance loss, called quadratic-inverse shrinkage (QIS). See Ledoit and Wolf (2021, Section 4.5)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O1FDF10QZ7Jv"},"outputs":[],"source":["#read available data\n","import pandas as pd\n","FF_u1 = pd.read_csv('data/FF_universe1.csv')\n","df =pd.read_csv('data/stocks100_u1.csv')\n","\n","FF_u2= pd.read_csv('data/FF_universe2.csv')\n","stocks_u2=pd.read_csv('data/stocks100_u2.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wflmOkRwl_OH"},"outputs":[],"source":["#print and inspect imported dataframe\n","df =pd.read_csv('data/stocks100_u1.csv')\n","print(df.head())\n","print(df.shape)"]},{"cell_type":"markdown","metadata":{"id":"gyNJuphkHfe-"},"source":["Implements the geometric-inverse shrinkage (GIS) estimator "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ucHZaE-417lc"},"outputs":[],"source":["# implementation  order\n","def calc_covar(df,K=None):\n","    print(\"===================================================================\\n\")\n","    print(\"Non-linear methods being calcualted:\\n\\n\")\n","    sigma_2_GIS = GIS(df, k=K)\n","    \"\"\"\n","    Explanation: called Geometric Inverse Shrinkage\n","      This is a nonlinear shrinkage estimator based on the Symmetrized \n","      Kullback-Leibler loss; it can be viewed as geometrically averaging\n","      linear-inverse shrinkage (LIS) with quadratic-inverse shrinkage (QIS)\n","\n","    \"\"\"\n","    print(\"covariance using GIS is calcualted\\n\")\n","    sigma_2_LIS = LIS(df, k=K)\n","    print(\"covariance using LIS is calcualted\\n\")\n","    sigma_2_QIS = QIS(df, k=K)\n","    print(\"covariance using QIS is calcualted\\n\\n\")\n","\n","    print(\"===================================================================\\n\")\n","    print(\"Linear methods being calcualted:\\n\\n\")\n","    sigma_2_cov1Para = cov1Para(df, k=K)\n","    print(\"covariance using 1 parameter is calcualted\")\n","    sigma_2_cov2Para = cov2Para(df, k=K)\n","    print(\"covariance using 2 parameter is calcualted\")\n","    sigma_2_covCor = covCor(df, k=K)\n","    print(\"covariance using correlation is calcualted\")\n","    sigma_2_covDiag = covDiag(df, k=K)\n","    print(\"covariance using Diagonal is calcualted\")\n","    sigma_2_covMarket = covMarket(df, k=K)\n","    print(\"covariance using Market is calcualted\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pou4EthPE-uY"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Oct  3 17:59:28 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","\n","# function sigmahat=GIS(Y,k) \n","#\n","# Y (N*p): raw data matrix of N iid observations on p random variables\n","# sigmahat (p*p): invertible covariance matrix estimator\n","#\n","# Implements the geometric-inverse shrinkage (GIS) estimator  \n","#    This is a nonlinear shrinkage estimator based on the Symmetrized\n","#    Kullback-Leibler loss; it can be viewed as geometrically averaging\n","#    linear-inverse shrinkage (LIS) with quadratic-inverse shrinkage (QIS)\n","#\n","# If the second (optional) parameter k is absent, not-a-number, or empty,\n","# then the algorithm demeans the data by default, and adjusts the effective\n","# sample size accordingly. If the user inputs k = 0, then no demeaning\n","# takes place; if (s)he inputs k = 1, then it signifies that the data Y has\n","# already been demeaned.\n","#\n","# This version: 01/2021\n","\n","### EXTRACT sample eigenvalues sorted in ascending order and eigenvectors ###\n","\n","#Imports\n","import numpy as np\n","import pandas as pd\n","import math\n","\n","#Sigmahat function\n","def GIS(Y,k=None):\n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","\n","    #Set df dimensions\n","    N = Y.shape[0]                                              #num of columns\n","    p = Y.shape[1]                                                 #num of rows\n","\n","    #default setting\n","    if (k is None or math.isnan(k)):\n","        Y = Y.sub(Y.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                      # adjust effective sample size\n","    # print(f\"N, sample size, total number of time periods of the sample: {N}\")\n","    # print(f\"p, paramters size, total number of assets (stocks chosen) in the sample: {p}\")\n","\n","    # print(f\"n, adjsuted sample size:{n}\")\n","    c = p/n                                               # concentration ratio\n","    # print(f\"c, concentration ratio:{c}\")\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    sample = (sample+sample.T)/2                              #make symmetrical\n","\n","    #Spectral decomp\n","    lambda1, u = np.linalg.eigh(sample)            #use Cholesky factorisation \n","    #                                               based on hermitian matrix\n","    lambda1 = lambda1.real.clip(min=0)              #reset negative values to 0\n","    dfu = pd.DataFrame(u,columns=lambda1)   #create df with column names lambda\n","    #                                        and values u\n","    dfu.sort_index(axis=1,inplace = True)              #sort df by column index\n","    lambda1 = dfu.columns                              #recapture sorted lambda\n","\n","    #COMPUTE Quadratic-Inverse Shrinkage estimator of the covariance matrix\n","    h = (min(c**2,1/c**2)**0.35)/p**0.35                   #smoothing parameter\n","    invlambda = 1/lambda1[max(1,p-n+1)-1:p]  #inverse of (non-null) eigenvalues\n","    dfl = pd.DataFrame()\n","    dfl['lambda'] = invlambda\n","    Lj = dfl[np.repeat(dfl.columns.values,min(p,n))]          #like  1/lambda_j\n","    Lj = pd.DataFrame(Lj.to_numpy())                        #Reset column names\n","    Lj_i = Lj.subtract(Lj.T)                    #like (1/lambda_j)-(1/lambda_i)\n","   \n","    theta = Lj.multiply(Lj_i).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)          #smoothed Stein shrinker\n","    Htheta = Lj.multiply(Lj*h).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)                    #its conjugate\n","    Atheta2 = theta**2+Htheta**2                         #its squared amplitude\n","    \n","    if p<=n:               #case where sample covariance matrix is not singular\n","        deltahat_1=(1-c)*invlambda+2*c*invlambda*theta #shrunk inverse eigenvalues (LIS)\n","        \n","        delta = 1 / ((1-c)**2*invlambda+2*c*(1-c)*invlambda*theta \\\n","                      +c**2*invlambda*Atheta2)    #optimally shrunk eigenvalues\n","        delta = delta.to_numpy()\n","    else: # case where sample covariance matrix is singular\n","        print('p must be <= n for the Symmetrized Kullback-Leibler divergence')       \n","        return -1\n","    \n","    temp = pd.DataFrame(deltahat_1)\n","    x = min(invlambda)\n","    temp.loc[temp[0] < x, 0] = x\n","    deltaLIS_1 = temp[0]\n","\n","    temp1 = dfu.to_numpy()\n","    temp2 = np.diag((delta/deltaLIS_1)**0.5)\n","    temp3 = dfu.T.to_numpy().conjugate()\n","    # reconstruct covariance matrix\n","    sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1,temp2),temp3))\n","    \n","    return sigmahat\n","\n","# df = pd.read_csv(r'C:\\Users\\Patrick Ledoit\\Documents\\Python\\translation\\input1.csv')\n","df =pd.read_csv('data/stocks100_u1.csv')\n","df = df.iloc[:,1:]\n","# df = df.T.reset_index().reset_index(drop=True)\n","\n","df = df.astype(float)\n","sigmahat = GIS(df,k=1)\n","display(sigmahat)"]},{"cell_type":"markdown","metadata":{"id":"3n97mLLWHpDl"},"source":["Implements the linear-inverse shrinkage (LIS) estimator  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbjb4xPMGUIk"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Oct  3 17:18:14 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","\n","# function sigmahat=LIS(Y,k) \n","#\n","# Y (N*p): raw data matrix of N iid observations on p random variables\n","# sigmahat (p*p): invertible covariance matrix estimator\n","#\n","# Implements the linear-inverse shrinkage (LIS) estimator  \n","#    This is a nonlinear shrinkage estimator derived under on Stein's loss\n","#\n","# If the second (optional) parameter k is absent, not-a-number, or empty,\n","# then the algorithm demeans the data by default, and adjusts the effective\n","# sample size accordingly. If the user inputs k = 0, then no demeaning\n","# takes place; if (s)he inputs k = 1, then it signifies that the data Y has\n","# already been demeaned.\n","#\n","# This version: 01/2021\n","\n","### EXTRACT sample eigenvalues sorted in ascending order and eigenvectors ###\n","\n","#Imports\n","import numpy as np\n","import pandas as pd\n","import math\n","\n","#Sigmahat function\n","def LIS(Y,k=None):\n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","\n","    #Set df dimensions\n","    N = Y.shape[0]                                              #num of columns\n","    p = Y.shape[1]                                                 #num of rows\n","\n","    #default setting\n","    if (k is None or math.isnan(k)):\n","        Y = Y.sub(Y.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                      # adjust effective sample size\n","    c = p/n                                               # concentration ratio\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    sample = (sample+sample.T)/2                              #make symmetrical\n","\n","    #Spectral decomp\n","    lambda1, u = np.linalg.eigh(sample)            #use Cholesky factorisation \n","    #                                               based on hermitian matrix\n","    lambda1 = lambda1.real.clip(min=0)              #reset negative values to 0\n","    dfu = pd.DataFrame(u,columns=lambda1)   #create df with column names lambda\n","    #                                        and values u\n","    dfu.sort_index(axis=1,inplace = True)              #sort df by column index\n","    lambda1 = dfu.columns                              #recapture sorted lambda\n","\n","    #COMPUTE Quadratic-Inverse Shrinkage estimator of the covariance matrix\n","    h = (min(c**2,1/c**2)**0.35)/p**0.35                   #smoothing parameter\n","    invlambda = 1/lambda1[max(1,p-n+1)-1:p]  #inverse of (non-null) eigenvalues\n","    dfl = pd.DataFrame()\n","    dfl['lambda'] = invlambda\n","    Lj = dfl[np.repeat(dfl.columns.values,min(p,n))]          #like  1/lambda_j\n","    Lj = pd.DataFrame(Lj.to_numpy())                        #Reset column names\n","    Lj_i = Lj.subtract(Lj.T)                    #like (1/lambda_j)-(1/lambda_i)\n","   \n","    theta = Lj.multiply(Lj_i).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)          #smoothed Stein shrinker\n","    \n","    if p<=n:               #case where sample covariance matrix is not singular\n","         deltahat_1=(1-c)*invlambda+2*c*invlambda*theta #shrunk inverse eigenvalues\n","         \n","    else: # case where sample covariance matrix is singular\n","        print(\"p must be <= n for Stein's loss\")       \n","        return -1\n","    \n","    temp = pd.DataFrame(deltahat_1)\n","    x = min(invlambda)\n","    temp.loc[temp[0] < x, 0] = x\n","    deltaLIS_1 = temp[0]\n","\n","\n","    temp1 = dfu.to_numpy()\n","    temp2 = np.diag(1/deltaLIS_1)\n","    temp3 = dfu.T.to_numpy().conjugate()\n","    # reconstruct covariance matrix\n","    sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1,temp2),temp3))\n","    \n","    return sigmahat\n","\n","df =pd.read_csv('data/stocks100_u1.csv')\n","df = df.iloc[:,1:]\n","# df = df.T.reset_index().T.reset_index(drop=True)\n","df = df.astype(float)\n","\n","sigmahat = LIS(df,k=1)\n","display(sigmahat)"]},{"cell_type":"markdown","metadata":{"id":"4JWnyDb_Ht1e"},"source":["Implements the quadratic-inverse shrinkage (QIS) estimator "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bAPBFyBKGvfj"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Sep 11 15:18:30 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","\n","#Imports\n","import numpy as np\n","import pandas as pd\n","import math\n","\n","# function sigmahat=QIS(Y,k) \n","#\n","# Y (N*p): raw data matrix of N iid observations on p random variables\n","# sigmahat (p*p): invertible covariance matrix estimator\n","#\n","# Implements the quadratic-inverse shrinkage (QIS) estimator  \n","#    This is a nonlinear shrinkage estimator derived under the Frobenius loss\n","#    and its two cousins, Inverse Stein's loss and Mininum Variance loss\n","#\n","# If the second (optional) parameter k is absent, not-a-number, or empty,\n","# then the algorithm demeans the data by default, and adjusts the effective\n","# sample size accordingly. If the user inputs k = 0, then no demeaning\n","# takes place; if (s)he inputs k = 1, then it signifies that the data Y has\n","# already been demeaned.\n","#\n","# This version: 01/2021\n","\n","def QIS(Y,k=None):\n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","\n","    #Set df dimensions\n","    N = Y.shape[0]                                              #num of columns\n","    p = Y.shape[1]                                                 #num of rows\n","\n","    #default setting\n","    if (k is None or math.isnan(k)):\n","        Y = Y.sub(Y.mean(axis=0), axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                      # adjust effective sample size\n","    c = p/n                                               # concentration ratio\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    sample = (sample+sample.T)/2                              #make symmetrical\n","\n","    #Spectral decomp\n","    lambda1, u = np.linalg.eigh(sample)            #use Cholesky factorisation \n","    #                                               based on hermitian matrix\n","    lambda1 = lambda1.real.clip(min=0)              #reset negative values to 0\n","    dfu = pd.DataFrame(u,columns=lambda1)   #create df with column names lambda\n","    #                                        and values u\n","    dfu.sort_index(axis=1,inplace = True)              #sort df by column index\n","    lambda1 = dfu.columns                              #recapture sorted lambda\n","\n","    #COMPUTE Quadratic-Inverse Shrinkage estimator of the covariance matrix\n","    h = (min(c**2,1/c**2)**0.35)/p**0.35                   #smoothing parameter\n","    invlambda = 1/lambda1[max(1,p-n+1)-1:p]  #inverse of (non-null) eigenvalues\n","    dfl = pd.DataFrame()\n","    dfl['lambda'] = invlambda\n","    Lj = dfl[np.repeat(dfl.columns.values,min(p,n))]          #like  1/lambda_j\n","    Lj = pd.DataFrame(Lj.to_numpy())                        #Reset column names\n","    Lj_i = Lj.subtract(Lj.T)                    #like (1/lambda_j)-(1/lambda_i)\n","   \n","    theta = Lj.multiply(Lj_i).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)          #smoothed Stein shrinker\n","    Htheta = Lj.multiply(Lj*h).div(Lj_i.multiply(Lj_i).add(\n","        Lj.multiply(Lj)*h**2)).mean(axis = 0)                    #its conjugate\n","    Atheta2 = theta**2+Htheta**2                         #its squared amplitude\n","\n","    if p<=n:               #case where sample covariance matrix is not singular\n","         delta = 1 / ((1-c)**2*invlambda+2*c*(1-c)*invlambda*theta \\\n","                      +c**2*invlambda*Atheta2)    #optimally shrunk eigenvalues\n","         delta = delta.to_numpy()\n","    else:\n","        delta0 = 1/((c-1)*np.mean(invlambda.to_numpy())) #shrinkage of null \n","        #                                                 eigenvalues\n","        delta = np.repeat(delta0,p-n)\n","        delta = np.concatenate((delta, 1/(invlambda*Atheta2)), axis=None)\n","\n","    deltaQIS = delta*(sum(lambda1)/sum(delta))                  #preserve trace\n","    \n","    temp1 = dfu.to_numpy()\n","    temp2 = np.diag(deltaQIS)\n","    temp3 = dfu.T.to_numpy().conjugate()\n","    #reconstruct covariance matrix\n","    sigmahat = pd.DataFrame(np.matmul(np.matmul(temp1,temp2),temp3))\n","    return sigmahat\n","\n","df =pd.read_csv('data/stocks100_u1.csv')\n","df = df.iloc[:,1:]\n","# df = df.T.reset_index().reset_index(drop=True)\n","df = df.astype(float)\n","\n","sigmahat = QIS(df,k=1)\n","display(sigmahat)"]},{"cell_type":"markdown","metadata":{"id":"LFP1xb3GHxn5"},"source":["cov1Para: Linear shrinkage towards one-parameter matrix; all the variances are the same, all the covariances are zero. See Ledoit and Wolf (2004b)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M4YpPCtMG4Jc"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Thu Jul  8 20:21:58 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","\n","# function sigmahat=cov1Para(Y,k)\n","#\n","# Y (N*p): raw data matrix of N iid observations on p random variables\n","# sigmahat (p*p): invertible covariance matrix estimator\n","#\n","# Shrinks towards one-parameter matrix:\n","#    all variances of the target are the same\n","#    all covariances of the target are zero\n","#\n","# If the second (optional) parameter k is absent, not-a-number, or empty,\n","# then the algorithm demeans the data by default, and adjusts the effective\n","# sample size accordingly. If the user inputs k = 0, then no demeaning\n","# takes place; if (s)he inputs k = 1, then it signifies that the data x has\n","# already been demeaned.\n","#\n","# This version: 01/2021, based on the 04/2014 version\n","\n","def cov1Para(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","    \n","    \n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    \n","    \n","    # compute shrinkage target\n","    diag = np.diag(sample.to_numpy())\n","    meanvar= sum(diag)/len(diag)\n","    target=meanvar*np.eye(p)\n","    \n","    \n","    \n","    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    \n","    \n","    pihat = sum(piMat.sum())\n","    \n","\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag=0;\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    rho_off=0;\n","    \n","    # compute shrinkage intensity\n","    rhohat=rho_diag+rho_off\n","    kappahat=(pihat-rhohat)/gammahat\n","    shrinkage=max(0,min(1,kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat=shrinkage*target+(1-shrinkage)*sample\n","    \n","    \n","    return sigmahat\n","\n","import pandas as pd\n","df =pd.read_csv('data/stocks100_u1.csv')\n","df = df.iloc[:,1:]\n","# df = df.T.reset_index().reset_index(drop=True)\n","df = df.astype(float)\n","\n","sigmahat = cov1Para(df)\n","display(sigmahat)"]},{"cell_type":"markdown","metadata":{"id":"cPv7nu_rH-o5"},"source":["cov2Para: Linear shrinkage towards two-parameter matrix; all the variances are the same, all the covariances are the same. See Ledoit (1995, Appendix B.1)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIC5NzxGG85f"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Aug 29 16:08:52 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","\n","# function sigmahat=cov2Para(Y,k)\n","#\n","# Y (N*p): raw data matrix of N iid observations on p random variables\n","# sigmahat (p*p): invertible covariance matrix estimator\n","#\n","# Shrinks towards two-parameter matrix:\n","#    all variances of the target are the same as one another\n","#    all covariances of the target are the same as one another\n","#\n","# If the second (optional) parameter k is absent, not-a-number, or empty,\n","# then the algorithm demeans the data by default, and adjusts the effective\n","# sample size accordingly. If the user inputs k = 0, then no demeaning\n","# takes place; if (s)he inputs k = 1, then it signifies that the data Y has\n","# already been demeaned.\n","#\n","# This version: 01/2021, based on the 06/2009 version\n","\n","\n","def cov2Para(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","    \n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","    \n","    \n","    #compute shrinkage target\n","    diag = np.diag(sample.to_numpy())\n","    meanvar= sum(diag)/len(diag)\n","    meancov = (np.sum(sample.to_numpy()) - np.sum(np.eye(p)*sample.to_numpy()))/(p*(p-1));\n","    target = pd.DataFrame(meanvar*np.eye(p)+meancov*(1-np.eye(p)))\n","    \n","    #estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    pihat = sum(piMat.sum())\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag = (sample2.sum().sum()-np.trace(sample.to_numpy())**2)/p;\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    sum1=Y.sum(axis=1)\n","    sum2=Y2.sum(axis=1)\n","    temp = (np.multiply(sum1.to_numpy(),sum1.to_numpy())-sum2)\n","    rho_off1 = np.sum(np.multiply(temp,temp))/(p*n)\n","    rho_off2 = (sample.sum().sum()-np.trace(sample.to_numpy()))**2/p\n","    rho_off = (rho_off1-rho_off2)/(p-1)\n","    \n","    # compute shrinkage intensity\n","    rhohat = rho_diag + rho_off\n","    kappahat = (pihat-rhohat) / gammahat\n","    shrinkage = max(0 , min(1 , kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat=shrinkage*target+(1-shrinkage)*sample\n","    \n","    return sigmahat\n","    \n","import pandas as pd\n","df =pd.read_csv('data/stocks100_u1.csv')\n","df = df.iloc[:,1:]\n","# df = df.T.reset_index().reset_index(drop=True)\n","df = df.astype(float)\n","\n","sigmahat = cov2Para(df)\n","display(sigmahat)"]},{"cell_type":"markdown","metadata":{"id":"w099KFAyIG9D"},"source":["covCor: Linear shrinkage towards constant-correlation matrix; the target preserves the diagonal of the sample covariance matrix and all correlation coefficients are the same. See Ledoit and Wolf (2004a)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1wkpakmHBy6"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Aug 29 17:08:18 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","\n","\n","# function sigmahat=covCor(Y,k)\n","#\n","# Y (N*p): raw data matrix of N iid observations on p random variables\n","# sigmahat (p*p): invertible covariance matrix estimator\n","#\n","# Shrinks towards constant-correlation matrix:\n","#    the target preserves the variances of the sample covariance matrix\n","#    all the correlation coefficients of the target are the same\n","#\n","# If the second (optional) parameter k is absent, not-a-number, or empty,\n","# then the algorithm demeans the data by default, and adjusts the effective\n","# sample size accordingly. If the user inputs k = 0, then no demeaning\n","# takes place; if (s)he inputs k = 1, then it signifies that the data Y has\n","# already been demeaned.\n","#\n","# This version: 01/2021, based on the 04/2014 version\n","\n","def covCor(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import numpy.matlib as mt\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","        \n","    # compute shrinkage target\n","    samplevar = np.diag(sample.to_numpy())\n","    sqrtvar = pd.DataFrame(np.sqrt(samplevar))\n","    rBar = (np.sum(np.sum(sample.to_numpy()/np.matmul(sqrtvar.to_numpy(),sqrtvar.T.to_numpy())))-p)/(p*(p-1)) # mean correlation\n","    target = pd.DataFrame(rBar*np.matmul(sqrtvar.to_numpy(),sqrtvar.T.to_numpy()))\n","    target[np.logical_and(np.eye(p),np.eye(p))] = sample[np.logical_and(np.eye(p),np.eye(p))];\n","    \n","    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    pihat = sum(piMat.sum())\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag =  np.sum(np.diag(piMat))\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    term1 = pd.DataFrame(np.matmul((Y**3).T.to_numpy(),Y.to_numpy())/n)\n","    term2 = pd.DataFrame(np.transpose(mt.repmat(samplevar,p,1))*sample)\n","    thetaMat = term1-term2\n","    thetaMat[np.logical_and(np.eye(p),np.eye(p))] = pd.DataFrame(np.zeros((p,p)))[np.logical_and(np.eye(p),np.eye(p))]\n","    rho_off = rBar*(np.matmul((1/sqrtvar).to_numpy(),sqrtvar.T.to_numpy())*thetaMat).sum().sum()\n","    \n","    # compute shrinkage intensity\n","    rhohat = rho_diag + rho_off\n","    kappahat = (pihat - rhohat) / gammahat\n","    shrinkage = max(0 , min(1 , kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat = shrinkage*target + (1-shrinkage) * sample;\n","    \n","    return sigmahat\n","\n","import pandas as pd\n","df =pd.read_csv('data/stocks100_u1.csv')\n","df = df.iloc[:,1:]\n","# df = df.T.reset_index().reset_index(drop=True)\n","df = df.astype(float)\n","\n","sigmahat = covCor(df)\n","display(sigmahat)"]},{"cell_type":"markdown","metadata":{"id":"kmeSeJIHIKAS"},"source":["covDiag: Linear shrinkage towards diagonal matrix; the target preserves the diagonal of the sample covariance matrix and all the covariances are zero. See Ledoit (1995, Appendix B.2)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UqaPPONnHF2y"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Sep 12 16:07:10 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","\n","\n","# function sigmahat=covCor(Y,k)\n","#\n","# Y (N*p): raw data matrix of N iid observations on p random variables\n","# sigmahat (p*p): invertible covariance matrix estimator\n","#\n","# Shrinks towards constant-correlation matrix:\n","#    the target preserves the variances of the sample covariance matrix\n","#    all the correlation coefficients of the target are the same\n","#\n","# If the second (optional) parameter k is absent, not-a-number, or empty,\n","# then the algorithm demeans the data by default, and adjusts the effective\n","# sample size accordingly. If the user inputs k = 0, then no demeaning\n","# takes place; if (s)he inputs k = 1, then it signifies that the data Y has\n","# already been demeaned.\n","#\n","# This version: 01/2021, based on the 04/2014 version\n","\n","def covDiag(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","\n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","        \n","    # compute shrinkage target\n","    target = pd.DataFrame(np.diag(np.diag(sample.to_numpy())))\n","    \n","    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    pihat = sum(piMat.sum())\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag =  np.sum(np.diag(piMat))\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    rho_off = 0\n","    \n","    # compute shrinkage intensity\n","    rhohat = rho_diag + rho_off\n","    kappahat = (pihat - rhohat) / gammahat\n","    shrinkage = max(0 , min(1 , kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat = shrinkage*target + (1-shrinkage) * sample;\n","    \n","    return sigmahat\n","\n","import pandas as pd\n","df =pd.read_csv('data/stocks100_u1.csv')\n","df = df.iloc[:,1:]\n","# df = df.T.reset_index().reset_index(drop=True)\n","df = df.astype(float)\n","\n","sigmahat = covDiag(df)\n","display(sigmahat)"]},{"cell_type":"markdown","metadata":{"id":"oPhb7Cu2IMvM"},"source":["covMarket: Linear shrinkage towards a one-factor market model, where the factor is defined as the cross-sectional average of all the random variables; thanks to the idiosyncratic volatility of the residuals, the target preserves the diagonal of the sample covariance matrix. See Ledoit and Wolf (2003)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EinJBSOSHJiB"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Sun Sep 12 16:17:27 2021\n","\n","@author: Patrick Ledoit\n","\"\"\"\n","def covMarket(Y,k = None):\n","    \n","    #Pre-Conditions: Y is a valid pd.dataframe and optional arg- k which can be\n","    #    None, np.nan or int\n","    #Post-Condition: Sigmahat dataframe is returned\n","    \n","    import numpy as np\n","    import numpy.matlib as mt\n","    import pandas as pd\n","    import math\n","\n","    # de-mean returns if required\n","    N,p = Y.shape                      # sample size and matrix dimension\n","   \n","    #default setting\n","    if k is None or math.isnan(k):\n","        \n","        mean = Y.mean(axis=0)\n","        Y = Y.sub(mean, axis=1)                               #demean\n","        k = 1\n","\n","    #vars\n","    n = N-k                                    # adjust effective sample size\n","    \n","    #Cov df: sample covariance matrix\n","    sample = pd.DataFrame(np.matmul(Y.T.to_numpy(),Y.to_numpy()))/n     \n","\n","    #compute shrinkage target\n","    Ymkt = Y.mean(axis = 1) #equal-weighted market factor\n","    covmkt = pd.DataFrame(np.matmul(Y.T.to_numpy(),Ymkt.to_numpy()))/n #covariance of original variables with common factor\n","    varmkt = np.matmul(Ymkt.T.to_numpy(),Ymkt.to_numpy())/n #variance of common factor\n","    target = pd.DataFrame(np.matmul(covmkt.to_numpy(),covmkt.T.to_numpy()))/varmkt\n","    target[np.logical_and(np.eye(p),np.eye(p))] = sample[np.logical_and(np.eye(p),np.eye(p))]\n","    \n","    # estimate the parameter that we call pi in Ledoit and Wolf (2003, JEF)\n","    Y2 = pd.DataFrame(np.multiply(Y.to_numpy(),Y.to_numpy()))\n","    sample2= pd.DataFrame(np.matmul(Y2.T.to_numpy(),Y2.to_numpy()))/n     # sample covariance matrix of squared returns\n","    piMat=pd.DataFrame(sample2.to_numpy()-np.multiply(sample.to_numpy(),sample.to_numpy()))\n","    pihat = sum(piMat.sum())\n","    \n","    # estimate the parameter that we call gamma in Ledoit and Wolf (2003, JEF)\n","    gammahat = np.linalg.norm(sample.to_numpy()-target,ord = 'fro')**2\n","    \n","    # diagonal part of the parameter that we call rho \n","    rho_diag =  np.sum(np.diag(piMat))\n","    \n","    # off-diagonal part of the parameter that we call rho \n","    temp = pd.DataFrame(Y.values*(pd.DataFrame([Ymkt for i in range(p)]).T.values)) #this must perform element wise multiplication but not performing #resolved\n","    covmktSQ = pd.DataFrame([covmkt[0] for i in range(p)])\n","    v1 = pd.DataFrame((1/n) * np.matmul(Y2.T.to_numpy(),temp.to_numpy())-np.multiply(covmktSQ.T.to_numpy(),sample.to_numpy()))\n","    roff1 = (np.sum(np.sum(np.multiply(v1.to_numpy(),covmktSQ.to_numpy())))-np.sum(np.diag(np.multiply(v1.to_numpy(),covmkt.to_numpy()))))/varmkt\n","    v3 = pd.DataFrame((1/n) * np.matmul(temp.T.to_numpy(),temp.to_numpy()) - varmkt * sample)\n","    roff3 = (np.sum(np.sum(np.multiply(v3.to_numpy(),np.matmul(covmkt.to_numpy(),covmkt.T.to_numpy())))) - np.sum(np.multiply(np.diag(v3.to_numpy()),(covmkt[0]**2).to_numpy()))) /varmkt**2\n","    rho_off=2*roff1-roff3\n","    \n","    # compute shrinkage intensity\n","    rhohat = rho_diag + rho_off\n","    kappahat = (pihat - rhohat) / gammahat\n","    shrinkage = max(0 , min(1 , kappahat/n))\n","    \n","    # compute shrinkage estimator\n","    sigmahat = shrinkage*target + (1-shrinkage) * sample;\n","    \n","    return sigmahat\n","    \n","import pandas as pd\n","df =pd.read_csv('data/stocks100_u1.csv')\n","df = df.iloc[:,1:]\n","# df = df.T.reset_index().reset_index(drop=True)\n","df = df.astype(float)\n","\n","sigmahat = covMarket(df,k=1)\n","display(sigmahat)"]},{"cell_type":"markdown","source":["## code with part of ledoit -wolf data 2017 experiment"],"metadata":{"id":"D7bvReE6UjOy"}},{"cell_type":"code","source":["##Package-----------------------------------\n","\n","import numpy as np\n","import pandas as pd\n","import nonlinshrink as nls\n","import time\n","\n","def showmatrixinfo(matrix):\n","    print(\"Matrix shape: \",np.shape(matrix))\n","    print(matrix)\n","    return\n","    \n","\n","##-------Test Function---------------------##\n","\n","def test_result(omega,method):\n","    \n","    ### Use estimated covariance matrix to calculate w\n","    Omega= np.matrix(omega)\n","    w=(Omega.I@np.ones((100,1)))/(np.ones((1,100))@Omega.I@np.ones((100,1)))\n","    \n","    ###########calculate payoff and excess payoff(ri-rf)\n","    \n","    payoff=w.T@yt_test*100\n","    \n","    exc_payoff=payoff-rf_daily*100\n","    print(\"\\t\"+method+\" complete!\")\n","    \n","    \n","    return exc_payoff.tolist()[0]\n","\n","##-------------Covariance Estimation----------------------##\n","\n","def run_1N():\n","    N=100\n","    omega=np.identity(100)\n","\n","    return1N=test_result(omega,\"1/N\")\n","    return return1N\n","\n","def run_Samp():\n","    ## ---------Sample Covariance Estimation\n","    omega=np.cov(yt_est)\n","\n","    returnSamp=test_result(omega,\"Samp\")\n","    return returnSamp\n","    \n","def run_Lin():\n","    ##--------Lin(Ledoit-Wolf 2004 methpd) shrinkage Estimation\n","    from sklearn.covariance import ledoit_wolf\n","\n","    omega=ledoit_wolf(yt_est.T)[0]\n","\n","    returnLin=test_result(omega,\"Lin\")\n","    return returnLin\n","\n","##-------NonLin shrinkage Estimation--------------\n","\n","def run_NonLin():\n","    import nonlinshrink as nls\n","\n","    omega=nls.shrink_cov(yt_est.T)\n","\n","    returnNonLin=test_result(omega,\"NonLin\")\n","    return returnNonLin\n","\n","##--------Single factor estimation---------------------##\n","\n","def run_SF():\n","    ##   Generate Factor\n","    equalw=np.array([[0.01]*100])\n","    factor=equalw@yt_est\n","\n","    ##estimate SigmaF\n","    var_f=np.var(factor,ddof=1)      #variance of factor,\n","\n","    np.savetxt(\"SF_factor.csv\",factor,delimiter=\",\")\n","\n","    ##compute cov(Ri,Rf),the covariance of stocks and factor\n","    var_if=np.cov(yt_est,factor)[-1,:-1]\n","    var_if=np.matrix(var_if)          #convert to 1X100 matrix\n","\n","\n","    SigmaSF=var_if.T*var_if/var_f  \n","    for i in range(100):\n","        SigmaSF[i,i]=np.cov(yt_est)[i,i]\n","\n","        \n","    returnSF=test_result(SigmaSF,\"SF\")\n","    return returnSF,SigmaSF\n","\n","##-----------------FAMA FRENCH estimation------------------------\n","\n","def run_FF():\n","    #####First, Generate 3-factors array.\n","\n","\n","    from sklearn.linear_model import LinearRegression\n","\n","    LG=LinearRegression()\n","\n","    LG.fit(FFfactors,yt_est.T)      ##FFfactor matirx is a (250,3) matrix!\n","    betas=LG.coef_\n","\n","    var_ff=np.cov(FFfactors.T)      ##Covariance of FAMA FRENCH 3 Factor model.\n","\n","    SigmaF=betas@var_ff@betas.T\n","\n","    ###As same as SF, the diagonal need add residual,or replace by var(Ri)\n","\n","    for i in range(100):\n","        SigmaF[i,i]=np.cov(yt_est)[i,i]\n","\n","    returnFF=test_result(SigmaF,\"FF\")\n","    return returnFF\n","\n","\n","def run_POET():\n","\n","#---------POET estimation-----------------#\n","    from sklearn.decomposition import PCA\n","    \n","    pca = PCA(n_components=5, copy=True)\n","    pca.fit(yt_est)\n","    factors=pca.components_\n","\n","\n","    ##Regression:\n","    from sklearn.linear_model import LinearRegression\n","\n","    LG2=LinearRegression()\n","\n","    LG2.fit(factors.T,yt_est.T)\n","    betas=LG2.coef_\n","\n","    var_fs=np.cov(factors)\n","\n","    SigmaF=betas@var_fs@betas.T\n","\n","    ###As same as SF, the diagonal need add residual,or replace by var(Ri)\n","\n","    for i in range(100):\n","        SigmaF[i,i]=np.cov(yt_est)[i,i]\n","        \n","        \n","    returnPOET=test_result(SigmaF,\"POET\")\n","    return returnPOET\n","\n","def run_NLSF(SigmaSF):\n","    ## NL-SF\n","    eigenvalue, eigenvectors = np.linalg.eig(SigmaSF)\n","    \n","    diag=np.identity(100)\n","    diag2=np.zeros((100,100))\n","    for i in range(100):\n","        diag[i,i]=pow(eigenvalue[i],-1/2)\n","        diag2[i,i]=pow(eigenvalue[i],1/2)\n","    ##Generate Yt x Sigma_SF to the power of -1/2\n","    SigmaSF2=eigenvectors@diag@eigenvectors.T  ##(1/2)\n","    SigmaSF3=eigenvectors@diag2@eigenvectors.T  ##(-1/2)\n","    SigmaC_hat=nls.shrink_cov(yt_est.T@SigmaSF2)\n","\n","    #Reincorporating the structure.\n","    SigmaNLSF=SigmaSF3@SigmaC_hat@SigmaSF3\n","    returnNLSF=test_result(SigmaNLSF,\"NL-SF\")\n","    return returnNLSF\n","\n","\n","##------------Initialize-------------------#\n","start=time.time()\n","\n","return1N=[]\n","returnSamp=[]\n","returnLin=[]\n","returnNonLin=[]\n","returnSF=[]\n","returnFF=[]\n","returnPOET=[]\n","returnNLSF=[]\n","\n","def run(fulldata):\n","    yt_est=fulldata[0]\n","    yt_test=fulldata[1]\n","    FFdata=fulldata[2]\n","    \n","    return1N.extend(run_1N())\n","    returnSamp.extend(run_Samp())\n","    returnLin.extend(run_Lin())\n","    returnNonLin.extend(run_NonLin())\n","    returnSF.extend(run_SF()[0])\n","    returnFF.extend(run_FF())\n","    returnPOET.extend(run_POET())\n","    returnNLSF.extend(run_NLSF(run_SF()[1]))\n","    \n","    return\n","\n","\n","#####--------------Running test----------------------------------#\n","\n","##------Data import-------------------------\n","FF_u1 = pd.read_csv('data/FF_universe1.csv')\n","stocks_u1=pd.read_csv('data/stocks100_u1.csv')\n","\n","FF_u2= pd.read_csv('data/FF_universe2.csv')\n","stocks_u2=pd.read_csv('data/stocks100_u2.csv')\n","\n","#-------------Iteration---------------------\n","\n","FF_u=FF_u1\n","stocks_u=stocks_u1\n","\n","for i in range(230):\n","    print(\"\\nTetst \"+str(i+1)+\" starts!\")\n","\n","    FFdata = FF_u.iloc[i*21:271+i*21,]\n","\n","    stocksdata=stocks_u.iloc[i*21:271+i*21,1:101].to_numpy()\n","    yt_est=stocksdata[:250,].T\n","    yt_test=stocksdata[250:,].T\n","\n","    index=[\"Mkt-RF\",\"SMB\",\"HML\"]\n","    FFfactors=FFdata.loc[:,index].iloc[:250,].to_numpy()\n","    rf_rate=FFdata.loc[:,\"RF\"].iloc[250:,].to_numpy()\n","    rf_daily=rf_rate/250                          \n","\n","    data=[yt_est,yt_test,FFdata]\n","    run(data)\n","    print(\"Test \"+str(i+1)+\" complete!\")\n","\n","\n","print(\"Universe 1 Complete!\")\n","\n","FF_u=FF_u2\n","stocks_u=stocks_u2\n","\n","for i in range(230):\n","    print(\"\\nTetst \"+str(i+231)+\" starts!\")\n","\n","    FFdata = FF_u.iloc[i*21:271+i*21,]\n","\n","    stocksdata=stocks_u.iloc[i*21:271+i*21,1:101].to_numpy()\n","    yt_est=stocksdata[:250,].T\n","    yt_test=stocksdata[250:,].T\n","\n","    index=[\"Mkt-RF\",\"SMB\",\"HML\"]\n","    FFfactors=FFdata.loc[:,index].iloc[:250,].to_numpy()\n","    rf_rate=FFdata.loc[:,\"RF\"].iloc[250:,].to_numpy()\n","    rf_daily=rf_rate/250                          \n","\n","    data=[yt_est,yt_test,FFdata]\n","    run(data)\n","    print(\"Test \"+str(i+231)+\" complete!\")\n","\n","\n","print(\"Universe 2 Complete!\")\n","\n","\n","\n","ret_mat=np.array([return1N,returnSamp,returnLin,returnNonLin,returnSF,returnFF,returnPOET,returnNLSF])\n","\n","print(\"Return maxtrix:\")\n","print(np.shape(ret_mat))\n","\n","\n","##----------------Calculating and plotting results------------------##\n","\n","AV=np.mean(ret_mat,axis=1)*250\n","AV=AV.T.tolist()\n","\n","SD=np.std(ret_mat,axis=1)*pow(250,.5)\n","SD=SD.T.tolist()\n","\n","SR=[]\n","for i in range(len(SD)):\n","    SR.append(AV[i]/SD[i])\n","\n","def prtb():\n","    print(\"Table 1\")\n","    print(\"Performance measures for various estimators of the GMV portfolio\")\n","    print(\"{}\".format('''Period: January 19, 1973 to December 31, 2011\" '''))\n","    print(\"\\t1/N \\tSample \\tLin \\tNolin \\tSF \\tFF \\tPOET \\tNL-SF\")\n","    print(\"{:-^70}\".format(\"\"))\n","    print(\"{:^70}\".format(\"N=100\"))\n","    print(\"{:-^70}\".format(\"\"))\n","    print(\"AV \\t{:.2f} \\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(\n","        AV[0],AV[1],AV[2],AV[3],AV[4],AV[5],AV[6],AV[7])\n","    )\n","    print(\"SD \\t{:.2f} \\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.1f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(\n","        SD[0],SD[1],SD[2],SD[3],SD[4],SD[5],SD[6],SD[7]))\n","    print(\"SR \\t{:.2f} \\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\\t{:.2f}\".format(\n","        SR[0],SR[1],SR[2],SR[3],SR[4],SR[5],SR[6],SR[7]))\n","    print(\"{:-^70}\".format(\"\"))\n","\n","        \n","    return\n","\n","\n","prtb()\n","\n","end=time.time()\n","\n","tm=end-start\n","print(\"Total running time:\"+str(tm)+\" Seconds\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RINQNjeUS4j","executionInfo":{"status":"ok","timestamp":1682978662153,"user_tz":-120,"elapsed":173870,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"}},"outputId":"41e0e6ff-69ce-4c7e-c253-a0d3869449b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 45 complete!\n","\n","Tetst 46 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 46 complete!\n","\n","Tetst 47 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 47 complete!\n","\n","Tetst 48 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 48 complete!\n","\n","Tetst 49 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 49 complete!\n","\n","Tetst 50 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 50 complete!\n","\n","Tetst 51 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 51 complete!\n","\n","Tetst 52 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 52 complete!\n","\n","Tetst 53 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 53 complete!\n","\n","Tetst 54 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 54 complete!\n","\n","Tetst 55 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 55 complete!\n","\n","Tetst 56 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 56 complete!\n","\n","Tetst 57 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 57 complete!\n","\n","Tetst 58 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 58 complete!\n","\n","Tetst 59 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 59 complete!\n","\n","Tetst 60 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 60 complete!\n","\n","Tetst 61 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 61 complete!\n","\n","Tetst 62 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 62 complete!\n","\n","Tetst 63 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 63 complete!\n","\n","Tetst 64 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 64 complete!\n","\n","Tetst 65 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 65 complete!\n","\n","Tetst 66 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 66 complete!\n","\n","Tetst 67 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 67 complete!\n","\n","Tetst 68 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 68 complete!\n","\n","Tetst 69 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 69 complete!\n","\n","Tetst 70 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 70 complete!\n","\n","Tetst 71 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 71 complete!\n","\n","Tetst 72 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 72 complete!\n","\n","Tetst 73 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 73 complete!\n","\n","Tetst 74 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 74 complete!\n","\n","Tetst 75 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 75 complete!\n","\n","Tetst 76 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 76 complete!\n","\n","Tetst 77 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 77 complete!\n","\n","Tetst 78 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 78 complete!\n","\n","Tetst 79 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 79 complete!\n","\n","Tetst 80 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 80 complete!\n","\n","Tetst 81 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 81 complete!\n","\n","Tetst 82 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 82 complete!\n","\n","Tetst 83 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 83 complete!\n","\n","Tetst 84 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 84 complete!\n","\n","Tetst 85 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 85 complete!\n","\n","Tetst 86 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 86 complete!\n","\n","Tetst 87 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 87 complete!\n","\n","Tetst 88 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 88 complete!\n","\n","Tetst 89 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 89 complete!\n","\n","Tetst 90 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 90 complete!\n","\n","Tetst 91 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 91 complete!\n","\n","Tetst 92 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 92 complete!\n","\n","Tetst 93 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 93 complete!\n","\n","Tetst 94 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 94 complete!\n","\n","Tetst 95 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 95 complete!\n","\n","Tetst 96 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 96 complete!\n","\n","Tetst 97 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 97 complete!\n","\n","Tetst 98 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 98 complete!\n","\n","Tetst 99 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 99 complete!\n","\n","Tetst 100 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 100 complete!\n","\n","Tetst 101 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 101 complete!\n","\n","Tetst 102 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 102 complete!\n","\n","Tetst 103 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 103 complete!\n","\n","Tetst 104 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 104 complete!\n","\n","Tetst 105 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 105 complete!\n","\n","Tetst 106 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 106 complete!\n","\n","Tetst 107 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 107 complete!\n","\n","Tetst 108 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 108 complete!\n","\n","Tetst 109 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 109 complete!\n","\n","Tetst 110 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 110 complete!\n","\n","Tetst 111 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 111 complete!\n","\n","Tetst 112 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 112 complete!\n","\n","Tetst 113 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 113 complete!\n","\n","Tetst 114 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 114 complete!\n","\n","Tetst 115 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 115 complete!\n","\n","Tetst 116 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 116 complete!\n","\n","Tetst 117 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 117 complete!\n","\n","Tetst 118 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 118 complete!\n","\n","Tetst 119 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 119 complete!\n","\n","Tetst 120 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 120 complete!\n","\n","Tetst 121 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 121 complete!\n","\n","Tetst 122 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 122 complete!\n","\n","Tetst 123 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 123 complete!\n","\n","Tetst 124 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 124 complete!\n","\n","Tetst 125 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 125 complete!\n","\n","Tetst 126 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 126 complete!\n","\n","Tetst 127 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 127 complete!\n","\n","Tetst 128 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 128 complete!\n","\n","Tetst 129 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 129 complete!\n","\n","Tetst 130 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 130 complete!\n","\n","Tetst 131 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 131 complete!\n","\n","Tetst 132 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 132 complete!\n","\n","Tetst 133 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 133 complete!\n","\n","Tetst 134 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 134 complete!\n","\n","Tetst 135 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 135 complete!\n","\n","Tetst 136 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 136 complete!\n","\n","Tetst 137 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 137 complete!\n","\n","Tetst 138 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 138 complete!\n","\n","Tetst 139 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 139 complete!\n","\n","Tetst 140 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 140 complete!\n","\n","Tetst 141 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 141 complete!\n","\n","Tetst 142 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 142 complete!\n","\n","Tetst 143 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 143 complete!\n","\n","Tetst 144 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 144 complete!\n","\n","Tetst 145 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 145 complete!\n","\n","Tetst 146 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 146 complete!\n","\n","Tetst 147 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 147 complete!\n","\n","Tetst 148 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 148 complete!\n","\n","Tetst 149 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 149 complete!\n","\n","Tetst 150 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 150 complete!\n","\n","Tetst 151 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 151 complete!\n","\n","Tetst 152 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 152 complete!\n","\n","Tetst 153 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 153 complete!\n","\n","Tetst 154 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 154 complete!\n","\n","Tetst 155 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 155 complete!\n","\n","Tetst 156 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 156 complete!\n","\n","Tetst 157 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 157 complete!\n","\n","Tetst 158 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 158 complete!\n","\n","Tetst 159 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 159 complete!\n","\n","Tetst 160 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 160 complete!\n","\n","Tetst 161 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 161 complete!\n","\n","Tetst 162 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 162 complete!\n","\n","Tetst 163 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 163 complete!\n","\n","Tetst 164 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 164 complete!\n","\n","Tetst 165 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 165 complete!\n","\n","Tetst 166 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 166 complete!\n","\n","Tetst 167 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 167 complete!\n","\n","Tetst 168 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 168 complete!\n","\n","Tetst 169 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 169 complete!\n","\n","Tetst 170 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 170 complete!\n","\n","Tetst 171 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 171 complete!\n","\n","Tetst 172 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 172 complete!\n","\n","Tetst 173 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 173 complete!\n","\n","Tetst 174 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 174 complete!\n","\n","Tetst 175 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 175 complete!\n","\n","Tetst 176 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 176 complete!\n","\n","Tetst 177 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 177 complete!\n","\n","Tetst 178 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 178 complete!\n","\n","Tetst 179 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 179 complete!\n","\n","Tetst 180 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 180 complete!\n","\n","Tetst 181 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 181 complete!\n","\n","Tetst 182 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 182 complete!\n","\n","Tetst 183 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 183 complete!\n","\n","Tetst 184 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 184 complete!\n","\n","Tetst 185 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 185 complete!\n","\n","Tetst 186 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 186 complete!\n","\n","Tetst 187 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 187 complete!\n","\n","Tetst 188 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 188 complete!\n","\n","Tetst 189 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 189 complete!\n","\n","Tetst 190 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 190 complete!\n","\n","Tetst 191 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 191 complete!\n","\n","Tetst 192 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 192 complete!\n","\n","Tetst 193 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 193 complete!\n","\n","Tetst 194 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 194 complete!\n","\n","Tetst 195 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 195 complete!\n","\n","Tetst 196 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 196 complete!\n","\n","Tetst 197 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 197 complete!\n","\n","Tetst 198 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 198 complete!\n","\n","Tetst 199 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 199 complete!\n","\n","Tetst 200 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 200 complete!\n","\n","Tetst 201 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 201 complete!\n","\n","Tetst 202 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 202 complete!\n","\n","Tetst 203 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 203 complete!\n","\n","Tetst 204 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 204 complete!\n","\n","Tetst 205 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 205 complete!\n","\n","Tetst 206 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 206 complete!\n","\n","Tetst 207 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 207 complete!\n","\n","Tetst 208 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 208 complete!\n","\n","Tetst 209 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 209 complete!\n","\n","Tetst 210 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 210 complete!\n","\n","Tetst 211 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 211 complete!\n","\n","Tetst 212 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 212 complete!\n","\n","Tetst 213 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 213 complete!\n","\n","Tetst 214 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 214 complete!\n","\n","Tetst 215 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 215 complete!\n","\n","Tetst 216 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 216 complete!\n","\n","Tetst 217 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 217 complete!\n","\n","Tetst 218 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 218 complete!\n","\n","Tetst 219 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 219 complete!\n","\n","Tetst 220 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 220 complete!\n","\n","Tetst 221 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 221 complete!\n","\n","Tetst 222 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 222 complete!\n","\n","Tetst 223 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 223 complete!\n","\n","Tetst 224 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 224 complete!\n","\n","Tetst 225 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 225 complete!\n","\n","Tetst 226 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 226 complete!\n","\n","Tetst 227 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 227 complete!\n","\n","Tetst 228 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 228 complete!\n","\n","Tetst 229 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 229 complete!\n","\n","Tetst 230 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 230 complete!\n","Universe 1 Complete!\n","\n","Tetst 231 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 231 complete!\n","\n","Tetst 232 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 232 complete!\n","\n","Tetst 233 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 233 complete!\n","\n","Tetst 234 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 234 complete!\n","\n","Tetst 235 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 235 complete!\n","\n","Tetst 236 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 236 complete!\n","\n","Tetst 237 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 237 complete!\n","\n","Tetst 238 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 238 complete!\n","\n","Tetst 239 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 239 complete!\n","\n","Tetst 240 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 240 complete!\n","\n","Tetst 241 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 241 complete!\n","\n","Tetst 242 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 242 complete!\n","\n","Tetst 243 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 243 complete!\n","\n","Tetst 244 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 244 complete!\n","\n","Tetst 245 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 245 complete!\n","\n","Tetst 246 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 246 complete!\n","\n","Tetst 247 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 247 complete!\n","\n","Tetst 248 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 248 complete!\n","\n","Tetst 249 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 249 complete!\n","\n","Tetst 250 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 250 complete!\n","\n","Tetst 251 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 251 complete!\n","\n","Tetst 252 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 252 complete!\n","\n","Tetst 253 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 253 complete!\n","\n","Tetst 254 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 254 complete!\n","\n","Tetst 255 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 255 complete!\n","\n","Tetst 256 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 256 complete!\n","\n","Tetst 257 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 257 complete!\n","\n","Tetst 258 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 258 complete!\n","\n","Tetst 259 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 259 complete!\n","\n","Tetst 260 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 260 complete!\n","\n","Tetst 261 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 261 complete!\n","\n","Tetst 262 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 262 complete!\n","\n","Tetst 263 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 263 complete!\n","\n","Tetst 264 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 264 complete!\n","\n","Tetst 265 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 265 complete!\n","\n","Tetst 266 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 266 complete!\n","\n","Tetst 267 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 267 complete!\n","\n","Tetst 268 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 268 complete!\n","\n","Tetst 269 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 269 complete!\n","\n","Tetst 270 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 270 complete!\n","\n","Tetst 271 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 271 complete!\n","\n","Tetst 272 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 272 complete!\n","\n","Tetst 273 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 273 complete!\n","\n","Tetst 274 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 274 complete!\n","\n","Tetst 275 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 275 complete!\n","\n","Tetst 276 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 276 complete!\n","\n","Tetst 277 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 277 complete!\n","\n","Tetst 278 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 278 complete!\n","\n","Tetst 279 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 279 complete!\n","\n","Tetst 280 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 280 complete!\n","\n","Tetst 281 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 281 complete!\n","\n","Tetst 282 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 282 complete!\n","\n","Tetst 283 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 283 complete!\n","\n","Tetst 284 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 284 complete!\n","\n","Tetst 285 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 285 complete!\n","\n","Tetst 286 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 286 complete!\n","\n","Tetst 287 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 287 complete!\n","\n","Tetst 288 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 288 complete!\n","\n","Tetst 289 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 289 complete!\n","\n","Tetst 290 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 290 complete!\n","\n","Tetst 291 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 291 complete!\n","\n","Tetst 292 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 292 complete!\n","\n","Tetst 293 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 293 complete!\n","\n","Tetst 294 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 294 complete!\n","\n","Tetst 295 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 295 complete!\n","\n","Tetst 296 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 296 complete!\n","\n","Tetst 297 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 297 complete!\n","\n","Tetst 298 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 298 complete!\n","\n","Tetst 299 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 299 complete!\n","\n","Tetst 300 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 300 complete!\n","\n","Tetst 301 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 301 complete!\n","\n","Tetst 302 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 302 complete!\n","\n","Tetst 303 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 303 complete!\n","\n","Tetst 304 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 304 complete!\n","\n","Tetst 305 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 305 complete!\n","\n","Tetst 306 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 306 complete!\n","\n","Tetst 307 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 307 complete!\n","\n","Tetst 308 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 308 complete!\n","\n","Tetst 309 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 309 complete!\n","\n","Tetst 310 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 310 complete!\n","\n","Tetst 311 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 311 complete!\n","\n","Tetst 312 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 312 complete!\n","\n","Tetst 313 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 313 complete!\n","\n","Tetst 314 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 314 complete!\n","\n","Tetst 315 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 315 complete!\n","\n","Tetst 316 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 316 complete!\n","\n","Tetst 317 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 317 complete!\n","\n","Tetst 318 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 318 complete!\n","\n","Tetst 319 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 319 complete!\n","\n","Tetst 320 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 320 complete!\n","\n","Tetst 321 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 321 complete!\n","\n","Tetst 322 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 322 complete!\n","\n","Tetst 323 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 323 complete!\n","\n","Tetst 324 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 324 complete!\n","\n","Tetst 325 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 325 complete!\n","\n","Tetst 326 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 326 complete!\n","\n","Tetst 327 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 327 complete!\n","\n","Tetst 328 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 328 complete!\n","\n","Tetst 329 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 329 complete!\n","\n","Tetst 330 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 330 complete!\n","\n","Tetst 331 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 331 complete!\n","\n","Tetst 332 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 332 complete!\n","\n","Tetst 333 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 333 complete!\n","\n","Tetst 334 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 334 complete!\n","\n","Tetst 335 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 335 complete!\n","\n","Tetst 336 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 336 complete!\n","\n","Tetst 337 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 337 complete!\n","\n","Tetst 338 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 338 complete!\n","\n","Tetst 339 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 339 complete!\n","\n","Tetst 340 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 340 complete!\n","\n","Tetst 341 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 341 complete!\n","\n","Tetst 342 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 342 complete!\n","\n","Tetst 343 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 343 complete!\n","\n","Tetst 344 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 344 complete!\n","\n","Tetst 345 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 345 complete!\n","\n","Tetst 346 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 346 complete!\n","\n","Tetst 347 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 347 complete!\n","\n","Tetst 348 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 348 complete!\n","\n","Tetst 349 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 349 complete!\n","\n","Tetst 350 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 350 complete!\n","\n","Tetst 351 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 351 complete!\n","\n","Tetst 352 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 352 complete!\n","\n","Tetst 353 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 353 complete!\n","\n","Tetst 354 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 354 complete!\n","\n","Tetst 355 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 355 complete!\n","\n","Tetst 356 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 356 complete!\n","\n","Tetst 357 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 357 complete!\n","\n","Tetst 358 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 358 complete!\n","\n","Tetst 359 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 359 complete!\n","\n","Tetst 360 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 360 complete!\n","\n","Tetst 361 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 361 complete!\n","\n","Tetst 362 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 362 complete!\n","\n","Tetst 363 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 363 complete!\n","\n","Tetst 364 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 364 complete!\n","\n","Tetst 365 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 365 complete!\n","\n","Tetst 366 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 366 complete!\n","\n","Tetst 367 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 367 complete!\n","\n","Tetst 368 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 368 complete!\n","\n","Tetst 369 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 369 complete!\n","\n","Tetst 370 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 370 complete!\n","\n","Tetst 371 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 371 complete!\n","\n","Tetst 372 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 372 complete!\n","\n","Tetst 373 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 373 complete!\n","\n","Tetst 374 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 374 complete!\n","\n","Tetst 375 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 375 complete!\n","\n","Tetst 376 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 376 complete!\n","\n","Tetst 377 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 377 complete!\n","\n","Tetst 378 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 378 complete!\n","\n","Tetst 379 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 379 complete!\n","\n","Tetst 380 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 380 complete!\n","\n","Tetst 381 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 381 complete!\n","\n","Tetst 382 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 382 complete!\n","\n","Tetst 383 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 383 complete!\n","\n","Tetst 384 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 384 complete!\n","\n","Tetst 385 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 385 complete!\n","\n","Tetst 386 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 386 complete!\n","\n","Tetst 387 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 387 complete!\n","\n","Tetst 388 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 388 complete!\n","\n","Tetst 389 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 389 complete!\n","\n","Tetst 390 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 390 complete!\n","\n","Tetst 391 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 391 complete!\n","\n","Tetst 392 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 392 complete!\n","\n","Tetst 393 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 393 complete!\n","\n","Tetst 394 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 394 complete!\n","\n","Tetst 395 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 395 complete!\n","\n","Tetst 396 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 396 complete!\n","\n","Tetst 397 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 397 complete!\n","\n","Tetst 398 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 398 complete!\n","\n","Tetst 399 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 399 complete!\n","\n","Tetst 400 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 400 complete!\n","\n","Tetst 401 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 401 complete!\n","\n","Tetst 402 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 402 complete!\n","\n","Tetst 403 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 403 complete!\n","\n","Tetst 404 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 404 complete!\n","\n","Tetst 405 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 405 complete!\n","\n","Tetst 406 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 406 complete!\n","\n","Tetst 407 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 407 complete!\n","\n","Tetst 408 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 408 complete!\n","\n","Tetst 409 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 409 complete!\n","\n","Tetst 410 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 410 complete!\n","\n","Tetst 411 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 411 complete!\n","\n","Tetst 412 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 412 complete!\n","\n","Tetst 413 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 413 complete!\n","\n","Tetst 414 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 414 complete!\n","\n","Tetst 415 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 415 complete!\n","\n","Tetst 416 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 416 complete!\n","\n","Tetst 417 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 417 complete!\n","\n","Tetst 418 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 418 complete!\n","\n","Tetst 419 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 419 complete!\n","\n","Tetst 420 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 420 complete!\n","\n","Tetst 421 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 421 complete!\n","\n","Tetst 422 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 422 complete!\n","\n","Tetst 423 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 423 complete!\n","\n","Tetst 424 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 424 complete!\n","\n","Tetst 425 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 425 complete!\n","\n","Tetst 426 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 426 complete!\n","\n","Tetst 427 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 427 complete!\n","\n","Tetst 428 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 428 complete!\n","\n","Tetst 429 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 429 complete!\n","\n","Tetst 430 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 430 complete!\n","\n","Tetst 431 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 431 complete!\n","\n","Tetst 432 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 432 complete!\n","\n","Tetst 433 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 433 complete!\n","\n","Tetst 434 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 434 complete!\n","\n","Tetst 435 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 435 complete!\n","\n","Tetst 436 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 436 complete!\n","\n","Tetst 437 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 437 complete!\n","\n","Tetst 438 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 438 complete!\n","\n","Tetst 439 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 439 complete!\n","\n","Tetst 440 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 440 complete!\n","\n","Tetst 441 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 441 complete!\n","\n","Tetst 442 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 442 complete!\n","\n","Tetst 443 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 443 complete!\n","\n","Tetst 444 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 444 complete!\n","\n","Tetst 445 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 445 complete!\n","\n","Tetst 446 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 446 complete!\n","\n","Tetst 447 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 447 complete!\n","\n","Tetst 448 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 448 complete!\n","\n","Tetst 449 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 449 complete!\n","\n","Tetst 450 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 450 complete!\n","\n","Tetst 451 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 451 complete!\n","\n","Tetst 452 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 452 complete!\n","\n","Tetst 453 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 453 complete!\n","\n","Tetst 454 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 454 complete!\n","\n","Tetst 455 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 455 complete!\n","\n","Tetst 456 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 456 complete!\n","\n","Tetst 457 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 457 complete!\n","\n","Tetst 458 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 458 complete!\n","\n","Tetst 459 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 459 complete!\n","\n","Tetst 460 starts!\n","\t1/N complete!\n","\tSamp complete!\n","\tLin complete!\n","\tNonLin complete!\n","\tSF complete!\n","\tFF complete!\n","\tPOET complete!\n","\tSF complete!\n","\tNL-SF complete!\n","Test 460 complete!\n","Universe 2 Complete!\n","Return maxtrix:\n","(8, 9660)\n","Table 1\n","Performance measures for various estimators of the GMV portfolio\n","Period: January 19, 1973 to December 31, 2011\" \n","\t1/N \tSample \tLin \tNolin \tSF \tFF \tPOET \tNL-SF\n","----------------------------------------------------------------------\n","                                N=100                                 \n","----------------------------------------------------------------------\n","AV \t14.66 \t10.41\t10.86\t10.94\t8.13\t9.26\t10.22\t10.57\n","SD \t16.81 \t12.18\t11.15\t10.68\t11.8\t11.42\t11.64\t10.47\n","SR \t0.87 \t0.85\t0.97\t1.02\t0.69\t0.81\t0.88\t1.01\n","----------------------------------------------------------------------\n","Total running time:171.94567322731018 Seconds\n"]}]},{"cell_type":"markdown","metadata":{"id":"WJjkQ-kFkG-V"},"source":["# code to see R workings in Non-Linear Shrinkage directly"]},{"cell_type":"markdown","metadata":{"id":"QghHypUIkKPq"},"source":["> The following cell give the nonlinear shrinkage from R library which is assumed to be more accurate and uses the QUEsT working of Ledoit and Wolf. Python version comaprison for the same data provides a very slight deviation (measured in frobenius loss terms) in the order of 0.00016187904158080588.\n","\n","> While the error may be big or small based on how this will impact the overall estimation, I consciously ignore this and go ahead with python implemetation. If you want to get an accurate estimation, I suggest you to use the R implementation completely for the program. Even better, if you use Julia.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27384,"status":"ok","timestamp":1682865313267,"user":{"displayName":"Rohit Koonireddy","userId":"17353448389884117833"},"user_tz":-120},"id":"gk66fanhkBC3","outputId":"2e49bcce-369b-4d25-ee4e-96ddc573cf86"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Estimating population eigenvalues...          0         1         2         3         4         5         6   \\\n","0   0.000283  0.000114  0.000103  0.000100  0.000060  0.000098  0.000082   \n","1   0.000114  0.000343  0.000090  0.000110  0.000071  0.000099  0.000079   \n","2   0.000103  0.000090  0.000440  0.000122  0.000046  0.000093  0.000062   \n","3   0.000100  0.000110  0.000122  0.000445  0.000071  0.000104  0.000112   \n","4   0.000060  0.000071  0.000046  0.000071  0.000463  0.000084  0.000127   \n","..       ...       ...       ...       ...       ...       ...       ...   \n","95  0.000097  0.000081  0.000113  0.000089  0.000064  0.000093  0.000084   \n","96  0.000049  0.000065  0.000071  0.000050  0.000042  0.000058  0.000089   \n","97  0.000058  0.000070  0.000059  0.000076  0.000069  0.000070  0.000083   \n","98  0.000071  0.000073  0.000029  0.000102  0.000042  0.000102  0.000102   \n","99  0.000042  0.000035  0.000059  0.000055  0.000023  0.000052  0.000026   \n","\n","          7         8         9   ...        90        91        92        93  \\\n","0   0.000073  0.000104  0.000203  ...  0.000086  0.000112  0.000021  0.000056   \n","1   0.000049  0.000101  0.000202  ...  0.000080  0.000103  0.000015  0.000081   \n","2   0.000074  0.000086  0.000208  ...  0.000090  0.000092  0.000019  0.000070   \n","3   0.000064  0.000091  0.000224  ...  0.000073  0.000096  0.000033  0.000106   \n","4   0.000053  0.000044  0.000161  ...  0.000036  0.000055  0.000006  0.000100   \n","..       ...       ...       ...  ...       ...       ...       ...       ...   \n","95  0.000054  0.000093  0.000177  ...  0.000074  0.000113  0.000026  0.000062   \n","96  0.000037  0.000045  0.000110  ...  0.000055  0.000048  0.000021  0.000038   \n","97  0.000036  0.000061  0.000134  ...  0.000052  0.000070  0.000019  0.000075   \n","98  0.000060  0.000063  0.000106  ...  0.000037  0.000075  0.000022  0.000055   \n","99  0.000032  0.000045  0.000053  ...  0.000040  0.000050  0.000013  0.000047   \n","\n","          94        95        96        97        98        99  \n","0   0.000131  0.000097  0.000049  0.000058  0.000071  0.000042  \n","1   0.000146  0.000081  0.000065  0.000070  0.000073  0.000035  \n","2   0.000106  0.000113  0.000071  0.000059  0.000029  0.000059  \n","3   0.000116  0.000089  0.000050  0.000076  0.000102  0.000055  \n","4   0.000079  0.000064  0.000042  0.000069  0.000042  0.000023  \n","..       ...       ...       ...       ...       ...       ...  \n","95  0.000078  0.000359  0.000046  0.000061  0.000036  0.000050  \n","96  0.000068  0.000046  0.000318  0.000034  0.000049  0.000029  \n","97  0.000088  0.000061  0.000034  0.000246  0.000037  0.000022  \n","98  0.000056  0.000036  0.000049  0.000037  0.000625  0.000038  \n","99  0.000044  0.000050  0.000029  0.000022  0.000038  0.000285  \n","\n","[100 rows x 100 columns]\n"]}],"source":["!pip install rpy2 --upgrade\n","#import drive and change folders to current folder\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","os.chdir(\"/content/drive/MyDrive/time_series_analysis_2023/final_implementation\")\n","\n","\n","import pandas as pd\n","import numpy as np\n","\n","log_returns = pd.read_csv(\"SandP500_log_returns_27-01-1981_31-12-2022.csv\")\n","# print(f\"stock log returns:\\n {log_returns.head()}\")\n","\n","\n","import rpy2.robjects as ro\n","import rpy2.robjects.packages as rpackages\n","from rpy2.robjects import pandas2ri, numpy2ri\n","pandas2ri.activate()\n","\n","# Load required R packages\n","nlshrink = rpackages.importr(\"nlshrink\")\n","df = log_returns.iloc[:252,1:101]\n","# Convert pandas DataFrame to numpy array\n","np_array = df.astype(float).to_numpy()\n","\n","# Convert numpy array to R matrix\n","r_matrix = numpy2ri.py2rpy(np_array)\n","\n","# Call the R function with the matrix as an argument\n","result = nlshrink.nlshrink_cov(r_matrix)\n","\n","# Convert the result back to a pandas DataFrame\n","result_df = pd.DataFrame(np.array(result))\n","\n","\n","# Print the result DataFrame\n","print(result_df)"]}],"metadata":{"colab":{"collapsed_sections":["WJjkQ-kFkG-V"],"provenance":[],"authorship_tag":"ABX9TyOvjPewO8AdHnl20Wa90KqH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}